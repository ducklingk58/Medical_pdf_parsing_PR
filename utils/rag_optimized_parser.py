"""
RAG ìµœì í™” PDF íŒŒì‹± ì‹œìŠ¤í…œ
ëª©í‘œ ìŠ¤í‚¤ë§ˆì— ë§ì¶˜ êµ¬ì¡°í™”ëœ JSON ì¶œë ¥ ìƒì„±
"""

import os
import re
import json
import uuid
import base64
import fitz  # PyMuPDF
import pandas as pd
import numpy as np
import cv2
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
from datetime import datetime
import logging
from PIL import Image
import io
import tempfile

# PaddleOCR imports
try:
    from paddleocr import PaddleOCR
    PADDLEOCR_AVAILABLE = True
except ImportError:
    PADDLEOCR_AVAILABLE = False
    print("Warning: PaddleOCR not available. OCR features will be disabled.")

# Unstructured imports
try:
    from unstructured.partition.auto import partition
    from unstructured.documents.elements import Table, Title, NarrativeText, Image as UnstructuredImage
    UNSTRUCTURED_AVAILABLE = True
except ImportError:
    UNSTRUCTURED_AVAILABLE = False
    print("Warning: Unstructured not available. Text extraction will be limited.")

# ê¸°ì¡´ ì„¤ì • import
try:
    from config.settings import Settings
except ImportError:
    # ê¸°ë³¸ ì„¤ì • í´ë˜ìŠ¤
    class Settings:
        SIMILARITY_THRESHOLD = 0.8
        SENTENCE_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
        REMOVE_PAGE_NUMBERS = True
        REMOVE_HEADERS_FOOTERS = True
        INCLUDE_IMAGE_METADATA = True
        PDF_EXTRACT_TABLES = True
        PDF_EXTRACT_IMAGES = True
        
        @staticmethod
        def create_directories():
            pass

class RAGOptimizedParser:
    """RAG ìµœì í™” JSON ìŠ¤í‚¤ë§ˆì— ë§ì¶˜ PDF íŒŒì‹±ê¸°"""
    
    def __init__(self, use_gpu: bool = False):
        self.logger = logging.getLogger(__name__)
        self.use_gpu = use_gpu
        
        # PaddleOCR ì´ˆê¸°í™” (ê°€ëŠ¥í•œ ê²½ìš°)
        self.ocr_system = self._init_paddle_ocr() if PADDLEOCR_AVAILABLE else None
        
        # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        self.temp_dir = Path(tempfile.mkdtemp())
        
        # ë¬¸ì„œ ë©”íƒ€ë°ì´í„°
        self.document_title = ""
        self.document_id = str(uuid.uuid4())
        self.source_pdf_filename = ""
        
        self.logger.info(f"RAG ìµœì í™” íŒŒì‹±ê¸° ì´ˆê¸°í™” ì™„ë£Œ (GPU: {use_gpu})")
    
    def _init_paddle_ocr(self) -> Optional[PaddleOCR]:
        """PaddleOCR ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        if not PADDLEOCR_AVAILABLE:
            return None
            
        try:
            # ê°€ì¥ ê¸°ë³¸ì ì¸ ì´ˆê¸°í™”
            ocr_system = PaddleOCR()  # ëª¨ë“  ë§¤ê°œë³€ìˆ˜ ì œê±°
            self.logger.info("PaddleOCR ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì„±ê³µ (ê¸°ë³¸ ì„¤ì •)")
            return ocr_system
        except Exception as e:
            self.logger.error(f"PaddleOCR ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            return None
    
    def extract_document_metadata(self, pdf_path: str) -> Dict[str, str]:
        """ë¬¸ì„œ ë ˆë²¨ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        try:
            self.source_pdf_filename = Path(pdf_path).name
            
            # PDF ë¬¸ì„œ ì—´ê¸°
            doc = fitz.open(pdf_path)
            
            # ì²« í˜ì´ì§€ì—ì„œ ì œëª© ì¶”ì¶œ ì‹œë„
            first_page = doc.load_page(0)
            text_blocks = first_page.get_text("dict")["blocks"]
            
            # ì œëª© í›„ë³´ë“¤ ì°¾ê¸° (í° í°íŠ¸, ìƒë‹¨ ìœ„ì¹˜)
            title_candidates = []
            for block in text_blocks:
                if "lines" in block:
                    for line in block["lines"]:
                        for span in line["spans"]:
                            font_size = span["size"]
                            y_pos = span["bbox"][1]  # y ì¢Œí‘œ
                            text = span["text"].strip()
                            
                            if font_size > 14 and y_pos < 200 and text:  # í° í°íŠ¸, ìƒë‹¨ ìœ„ì¹˜
                                title_candidates.append((text, font_size, y_pos))
            
            # ê°€ì¥ í° í°íŠ¸ì™€ ìƒë‹¨ì— ìœ„ì¹˜í•œ í…ìŠ¤íŠ¸ë¥¼ ì œëª©ìœ¼ë¡œ ì„ íƒ
            if title_candidates:
                title_candidates.sort(key=lambda x: (x[1], -x[2]), reverse=True)
                self.document_title = title_candidates[0][0]
            else:
                # ê¸°ë³¸ê°’ìœ¼ë¡œ íŒŒì¼ëª… ì‚¬ìš©
                self.document_title = Path(pdf_path).stem
            
            doc.close()
            
            return {
                "document_title": self.document_title,
                "document_id": self.document_id,
                "created_at": datetime.now().isoformat(),
                "source_pdf_filename": self.source_pdf_filename
            }
            
        except Exception as e:
            self.logger.error(f"ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
            return {
                "document_title": Path(pdf_path).stem,
                "document_id": self.document_id,
                "created_at": datetime.now().isoformat(),
                "source_pdf_filename": self.source_pdf_filename
            }
    
    def pdf_to_images(self, pdf_path: str, dpi: int = 300) -> List[Tuple[int, np.ndarray, Dict]]:
        """PDFë¥¼ ê³ í•´ìƒë„ ì´ë¯¸ì§€ë¡œ ë³€í™˜í•˜ê³  í˜ì´ì§€ ë©”íƒ€ë°ì´í„° í¬í•¨"""
        try:
            self.logger.info(f"PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜ ì¤‘: {pdf_path}")
            doc = fitz.open(pdf_path)
            images = []
            
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                
                # ê³ í•´ìƒë„ ì´ë¯¸ì§€ë¡œ ë Œë”ë§
                mat = fitz.Matrix(dpi/72, dpi/72)
                pix = page.get_pixmap(matrix=mat)
                
                # PIL Imageë¡œ ë³€í™˜
                img_data = pix.tobytes("png")
                pil_image = Image.open(io.BytesIO(img_data))
                
                # OpenCV í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                cv_image = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)
                
                # í˜ì´ì§€ ë©”íƒ€ë°ì´í„°
                page_metadata = {
                    "page_number": page_num + 1,
                    "width": pix.width,
                    "height": pix.height,
                    "rotation": page.rotation
                }
                
                images.append((page_num, cv_image, page_metadata))
                pix = None
            
            doc.close()
            self.logger.info(f"ì´ {len(images)}í˜ì´ì§€ ì´ë¯¸ì§€ ë³€í™˜ ì™„ë£Œ")
            return images
            
        except Exception as e:
            self.logger.error(f"PDF ì´ë¯¸ì§€ ë³€í™˜ ì˜¤ë¥˜: {str(e)}")
            return []  # ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ìœ¼ë¡œ ë³€ê²½
    
    def extract_text_chunks(self, pdf_path: str) -> List[Dict[str, Any]]:
        """í…ìŠ¤íŠ¸ ì²­í¬ ì¶”ì¶œ ë° ìƒì„¸ ë©”íƒ€ë°ì´í„° ìƒì„±"""
        try:
            self.logger.info("í…ìŠ¤íŠ¸ ì²­í¬ ì¶”ì¶œ ì‹œì‘")
            
            text_chunks = []
            doc = fitz.open(pdf_path)
            
            if UNSTRUCTURED_AVAILABLE:
                # Unstructuredë¡œ ê¸°ë³¸ íŒŒì‹±
                elements = partition(
                    filename=pdf_path,
                    include_image_metadata=True,
                    extract_tables=False,  # í‘œëŠ” ë³„ë„ë¡œ ì²˜ë¦¬
                    pdf_extract_images=False,  # ì´ë¯¸ì§€ëŠ” ë³„ë„ë¡œ ì²˜ë¦¬
                    strategy="fast"
                )
                
                for i, element in enumerate(elements):
                    if isinstance(element, (NarrativeText, Title)):
                        chunk_data = self._create_text_chunk(element, doc, i)
                        if chunk_data:
                            # í…ìŠ¤íŠ¸ ì²­í¬ì„ì„ ëª…ì‹œ
                            chunk_data["type"] = "text"
                            chunk_data["subtype"] = "narrative" if isinstance(element, NarrativeText) else "title"
                            text_chunks.append(chunk_data)
            else:
                # PyMuPDFë§Œ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                text_chunks = self._extract_text_with_pymupdf(doc)
                # ê¸°ë³¸ íƒ€ì… ì„¤ì • (ì´ë¯¸ _extract_text_with_pymupdfì—ì„œ ì„¤ì •ë¨)
                pass
            
            doc.close()
            self.logger.info(f"ì´ {len(text_chunks)}ê°œ í…ìŠ¤íŠ¸ ì²­í¬ ì¶”ì¶œ ì™„ë£Œ")
            return text_chunks
            
        except Exception as e:
            self.logger.error(f"í…ìŠ¤íŠ¸ ì²­í¬ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
            return []
    
    def _create_text_chunk(self, element, doc, index: int) -> Optional[Dict[str, Any]]:
        """Unstructured ìš”ì†Œë¡œë¶€í„° í…ìŠ¤íŠ¸ ì²­í¬ ìƒì„±"""
        try:
            chunk_id = str(uuid.uuid4())
            
            # ê¸°ë³¸ ì •ë³´
            content = element.text.strip()
            if not content:
                return None
            
            # ì¹´í…Œê³ ë¦¬ ê²°ì •
            category = self._determine_category(element)
            
            # í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ
            page_numbers = self._extract_page_numbers(element, doc)
            
            # ì¢Œí‘œ ì¶”ì¶œ (ê°€ëŠ¥í•œ ê²½ìš°)
            coordinates = self._extract_coordinates(element, doc)
            
            # ì„¹ì…˜ ì œëª© ì¶”ì¶œ
            section_title = self._extract_section_title(content, index)
            
            # í‘œ/ê·¸ë¦¼ ê´€ë ¨ì„± í™•ì¸
            is_table_related, is_figure_related = self._check_element_relation(content)
            
            chunk_data = {
                "chunk_id": chunk_id,
                "content": content,
                "type": "text",  # í…ìŠ¤íŠ¸ ì²­í¬ì„ì„ ëª…ì‹œ
                "subtype": "narrative" if isinstance(element, NarrativeText) else "title",
                "section_title": section_title,
                "category": category,
                "page_numbers": page_numbers,
                "coordinates": coordinates,
                "is_table_related": is_table_related,
                "is_figure_related": is_figure_related,
                "referenced_element_id": None,  # ë‚˜ì¤‘ì— ì—°ê²°
                "extraction_tool": "Unstructured",
                "confidence_score": 0.9  # ê¸°ë³¸ê°’
            }
            
            return chunk_data
            
        except Exception as e:
            self.logger.error(f"í…ìŠ¤íŠ¸ ì²­í¬ ìƒì„± ì˜¤ë¥˜: {str(e)}")
            return None
    
    def _extract_text_with_pymupdf(self, doc) -> List[Dict[str, Any]]:
        """PyMuPDFë§Œ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        text_chunks = []
        
        for page_num in range(len(doc)):
            page = doc.load_page(page_num)
            text_blocks = page.get_text("dict")["blocks"]
            
            for block in text_blocks:
                if "lines" in block:
                    for line in block["lines"]:
                        for span in line["spans"]:
                            text = span["text"].strip()
                            if text and len(text) > 10:  # ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ë§Œ
                                chunk_id = str(uuid.uuid4())
                                
                                # í°íŠ¸ í¬ê¸°ë¡œ ì¹´í…Œê³ ë¦¬ ê²°ì •
                                font_size = span["size"]
                                if font_size > 16:
                                    category = "Title"
                                elif font_size > 12:
                                    category = "Subtitle"
                                else:
                                    category = "NarrativeText"
                                
                                chunk_data = {
                                    "chunk_id": chunk_id,
                                    "content": text,
                                    "type": "text",  # í…ìŠ¤íŠ¸ ì²­í¬ì„ì„ ëª…ì‹œ
                                    "subtype": "pymupdf",
                                    "section_title": "",
                                    "category": category,
                                    "page_numbers": [page_num + 1],
                                    "coordinates": list(span["bbox"]),
                                    "is_table_related": "í‘œ" in text or "table" in text.lower(),
                                    "is_figure_related": "ê·¸ë¦¼" in text or "figure" in text.lower(),
                                    "referenced_element_id": None,
                                    "extraction_tool": "PyMuPDF",
                                    "confidence_score": 0.8
                                }
                                
                                text_chunks.append(chunk_data)
        
        return text_chunks
    
    def _determine_category(self, element) -> str:
        """ìš”ì†Œì˜ ì¹´í…Œê³ ë¦¬ ê²°ì •"""
        if isinstance(element, Title):
            return "Title"
        elif isinstance(element, NarrativeText):
            text = element.text.lower()
            if any(keyword in text for keyword in ["í‘œ", "table", "figure", "ê·¸ë¦¼"]):
                return "Caption"
            elif any(keyword in text for keyword in ["â€¢", "-", "1.", "2.", "3."]):
                return "ListItem"
            else:
                return "NarrativeText"
        else:
            return "Text"
    
    def _extract_page_numbers(self, element, doc) -> List[int]:
        """í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ"""
        try:
            # Unstructuredì˜ ë©”íƒ€ë°ì´í„°ì—ì„œ í˜ì´ì§€ ì •ë³´ ì¶”ì¶œ
            if hasattr(element, 'metadata') and hasattr(element.metadata, 'page_number'):
                return [element.metadata.page_number]
            
            # ê¸°ë³¸ê°’
            return [1]
        except:
            return [1]
    
    def _extract_coordinates(self, element, doc) -> List[float]:
        """ì¢Œí‘œ ì¶”ì¶œ (ê¸°ë³¸ê°’ ë°˜í™˜)"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” PyMuPDFë¥¼ ì‚¬ìš©í•˜ì—¬ ì •í™•í•œ ì¢Œí‘œ ì¶”ì¶œ
        return [0.0, 0.0, 100.0, 100.0]  # ê¸°ë³¸ê°’
    
    def _extract_section_title(self, content: str, index: int) -> str:
        """ì„¹ì…˜ ì œëª© ì¶”ì¶œ"""
        # ê°„ë‹¨í•œ íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ì„¹ì…˜ ì œëª© ì¶”ì¶œ
        lines = content.split('\n')
        for line in lines:
            line = line.strip()
            # ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” ì œëª© íŒ¨í„´
            if re.match(r'^\d+\.', line):
                return line
            # ëŒ€ë¬¸ìë¡œ ì‹œì‘í•˜ëŠ” ì§§ì€ ì œëª©
            elif len(line) < 50 and line.isupper():
                return line
        
        return ""
    
    def _check_element_relation(self, content: str) -> Tuple[bool, bool]:
        """í…ìŠ¤íŠ¸ê°€ í‘œë‚˜ ê·¸ë¦¼ê³¼ ê´€ë ¨ìˆëŠ”ì§€ í™•ì¸"""
        content_lower = content.lower()
        is_table_related = any(keyword in content_lower for keyword in ["í‘œ", "table", "í…Œì´ë¸”"])
        is_figure_related = any(keyword in content_lower for keyword in ["ê·¸ë¦¼", "figure", "ë„", "ì°¨íŠ¸"])
        return is_table_related, is_figure_related
    
    def extract_tables(self, pdf_path: str) -> List[Dict[str, Any]]:
        """í‘œ ë°ì´í„° ì¶”ì¶œ ë° êµ¬ì¡°í™” - Microsoft Table Transformer ì „ìš©"""
        try:
            self.logger.info("í‘œ ì¶”ì¶œ ì‹œì‘ (Microsoft Table Transformer)")
            
            tables = []
            
            # 1ì°¨: Table Transformer ì‹œë„ (ì£¼ ì—”ì§„)
            try:
                from utils.table_transformer_detector import TableTransformerDetector, TRANSFORMER_AVAILABLE
                
                if TRANSFORMER_AVAILABLE:
                    self.logger.info("ğŸš€ Microsoft Table Transformer í‘œ ê°ì§€ ì‹œì‘...")
                    detector = TableTransformerDetector()
                    tt_result = detector.detect_tables_in_pdf(pdf_path, confidence_threshold=0.5)  # ë‚®ì€ ì„ê³„ê°’ìœ¼ë¡œ ë” ë§ì´ ê°ì§€
                    
                    if tt_result["success"] and tt_result["tables"]:
                        self.logger.info(f"âœ… Table Transformer: {len(tt_result['tables'])}ê°œ í‘œ ê°ì§€")
                        
                        # ë‚´ìš©ê¹Œì§€ ì¶”ì¶œëœ ì™„ì „í•œ ê²°ê³¼ ì‚¬ìš©
                        enhanced_result = detector.process_pdf_with_table_transformer(pdf_path, confidence_threshold=0.5)
                        
                        if enhanced_result["success"] and enhanced_result["tables"]:
                            for table in enhanced_result["tables"]:
                                rag_table = {
                                    "table_id": table.get('table_id', str(uuid.uuid4())),
                                    "caption": f"í‘œ {table.get('page_number', 1)} (Table Transformer)",
                                    "page_number": table.get('page_number', 1),
                                    "coordinates": table.get('coordinates', []),
                                    "structure": {"rows": 0, "columns": 0},  # êµ¬ì¡°ëŠ” OCR ê²°ê³¼ë¡œ ì—…ë°ì´íŠ¸
                                    "table_data": table.get('table_data', []),
                                    "extracted_text": table.get('extracted_text', ''),
                                    "extraction_tool": "Microsoft_Table_Transformer",
                                    "confidence_score": table.get('confidence_score', 0.0),
                                    "bbox": table.get('bbox', {}),
                                    "cell_texts": table.get('cell_texts', []),
                                    "ocr_method": table.get('ocr_method', 'PaddleOCR'),
                                    "avg_confidence": table.get('avg_confidence', 0.0)
                                }
                                tables.append(rag_table)
                        
                        self.logger.info(f"Table Transformerë¡œ {len(tables)}ê°œ í‘œ ê°ì§€ ë° ë‚´ìš© ì¶”ì¶œ ì™„ë£Œ")
                        
                    else:
                        self.logger.info("Table Transformerì—ì„œ í‘œë¥¼ ê°ì§€í•˜ì§€ ëª»í•¨")
                else:
                    self.logger.warning("Table Transformer ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    self.logger.info("ì„¤ì¹˜ ëª…ë ¹: pip install transformers torch torchvision")
                    
            except Exception as e:
                self.logger.error(f"Table Transformer ì‹¤íŒ¨: {str(e)}")
            
            # 2ì°¨: íŒ¨í„´ ê¸°ë°˜ ê°€ìƒ í‘œ ìƒì„± (ë³´ì¡° ì—”ì§„)
            try:
                self.logger.info("ğŸ”„ êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ íŒ¨í„´ ê¸°ë°˜ ê°€ìƒ í‘œ ìƒì„±...")
                pattern_tables = self._extract_structured_text_as_tables(pdf_path)
                
                if pattern_tables:
                    self.logger.info(f"íŒ¨í„´ ê¸°ë°˜ìœ¼ë¡œ {len(pattern_tables)}ê°œ ê°€ìƒ í‘œ ìƒì„±")
                    tables.extend(pattern_tables)
                    
            except Exception as e:
                self.logger.warning(f"íŒ¨í„´ ê¸°ë°˜ í‘œ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            
            # ìµœì¢… ê²°ê³¼
            if tables:
                self.logger.info(f"ì´ {len(tables)}ê°œ í‘œ ì¶”ì¶œ ì™„ë£Œ (Table Transformer + íŒ¨í„´ ê¸°ë°˜)")
            else:
                self.logger.warning("ì–´ë–¤ ë°©ë²•ìœ¼ë¡œë„ í‘œë¥¼ ê°ì§€í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                
            return tables
            
        except Exception as e:
            self.logger.error(f"í‘œ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def _extract_table_text(self, table_data: List) -> str:
        """í…Œì´ë¸” ë°ì´í„°ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        if not table_data:
            return ""
        
        text_parts = []
        for row in table_data:
            if isinstance(row, list):
                for cell in row:
                    if cell and str(cell).strip():
                        text_parts.append(str(cell).strip())
            elif isinstance(row, str) and row.strip():
                text_parts.append(row.strip())
        
        return " ".join(text_parts)
    
    def _extract_tables_fallback(self, pdf_path: str) -> List[Dict[str, Any]]:
        """ê¸°ë³¸ í…Œì´ë¸” ì¶”ì¶œ ë°©ë²• (í´ë°±)"""
        try:
            self.logger.info("ê¸°ë³¸ í…Œì´ë¸” ì¶”ì¶œ ë°©ë²• ì‚¬ìš©")
            
            tables = []
            doc = fitz.open(pdf_path)
            
            # PyMuPDFë¥¼ ì‚¬ìš©í•œ ê°„ë‹¨í•œ í‘œ ê°ì§€
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                
                # í‘œ ê°ì§€ë¥¼ ìœ„í•œ ê°„ë‹¨í•œ ë°©ë²• (í…ìŠ¤íŠ¸ ë¸”ë¡ ë¶„ì„)
                text_blocks = page.get_text("dict")["blocks"]
                
                # í‘œ í›„ë³´ ì°¾ê¸° (ì •ë ¬ëœ í…ìŠ¤íŠ¸ ë¸”ë¡ë“¤)
                table_candidates = self._find_table_candidates(text_blocks, page_num)
                
                for i, candidate in enumerate(table_candidates):
                    table_id = str(uuid.uuid4())
                    
                    # ê°„ë‹¨í•œ í‘œ êµ¬ì¡° ìƒì„±
                    structure = self._create_simple_table_structure(candidate)
                    
                    table_data = {
                        "table_id": table_id,
                        "caption": f"í‘œ {page_num + 1}-{i + 1}",
                        "page_number": page_num + 1,
                        "coordinates": [0.0, 0.0, 100.0, 100.0],  # ê¸°ë³¸ê°’
                        "structure": structure,
                        "extracted_text": " ".join([cell for row in structure for cell in row if cell]),
                        "extraction_tool": "PyMuPDF_Simple",
                        "confidence_score": 0.7,
                        "file_path": None
                    }
                    
                    tables.append(table_data)
            
            doc.close()
            return tables
            
        except Exception as e:
            self.logger.error(f"ê¸°ë³¸ í‘œ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
            return []
    
    def _find_table_candidates(self, text_blocks, page_num: int) -> List[List]:
        """í…ìŠ¤íŠ¸ ë¸”ë¡ì—ì„œ í‘œ í›„ë³´ ì°¾ê¸°"""
        candidates = []
        
        # ê°„ë‹¨í•œ í‘œ ê°ì§€ ë¡œì§
        # ì—¬ëŸ¬ ì¤„ì— ê±¸ì³ ì •ë ¬ëœ í…ìŠ¤íŠ¸ ë¸”ë¡ë“¤ì„ í‘œë¡œ ê°„ì£¼
        lines = []
        for block in text_blocks:
            if "lines" in block:
                for line in block["lines"]:
                    line_text = " ".join([span["text"] for span in line["spans"]])
                    if line_text.strip():
                        lines.append(line_text.strip())
        
        # ì—°ì†ëœ ì¤„ë“¤ì„ í‘œ í›„ë³´ë¡œ ê°„ì£¼
        if len(lines) > 2:
            candidates.append(lines)
        
        return candidates
    
    def _create_simple_table_structure(self, lines: List[str]) -> List[List[str]]:
        """ê°„ë‹¨í•œ í‘œ êµ¬ì¡° ìƒì„±"""
        structure = []
        
        for line in lines:
            # íƒ­ì´ë‚˜ ì—¬ëŸ¬ ê³µë°±ìœ¼ë¡œ êµ¬ë¶„ëœ ì…€ë“¤
            cells = re.split(r'\t+|\s{2,}', line)
            cells = [cell.strip() for cell in cells if cell.strip()]
            if cells:
                structure.append(cells)
        
        return structure
    
    def extract_figures(self, pdf_path: str, output_dir: str) -> List[Dict[str, Any]]:
        """ê·¸ë¦¼ ë°ì´í„° ì¶”ì¶œ (ê°„ë‹¨í•œ ë²„ì „)"""
        try:
            self.logger.info("ê·¸ë¦¼ ì¶”ì¶œ ì‹œì‘")
            
            figures = []
            doc = fitz.open(pdf_path)
            
            # ì´ë¯¸ì§€ ì €ì¥ ë””ë ‰í† ë¦¬
            images_dir = Path(output_dir) / "images"
            images_dir.mkdir(parents=True, exist_ok=True)
            
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                
                # í˜ì´ì§€ì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ
                image_list = page.get_images()
                
                for img_index, img in enumerate(image_list):
                    figure_id = str(uuid.uuid4())
                    
                    try:
                        # ì´ë¯¸ì§€ ë°ì´í„° ì¶”ì¶œ
                        xref = img[0]
                        pix = fitz.Pixmap(doc, xref)
                        
                        # ì´ë¯¸ì§€ íŒŒì¼ ì €ì¥
                        image_filename = f"figure_{figure_id[:8]}.png"
                        image_path = images_dir / image_filename
                        pix.save(str(image_path))
                        
                        # Base64 ì¸ì½”ë”© (ì‘ì€ ì´ë¯¸ì§€ì˜ ê²½ìš°)
                        image_base64 = None
                        if pix.size < 1000000:  # 1MB ë¯¸ë§Œ
                            img_data = pix.tobytes("png")
                            image_base64 = base64.b64encode(img_data).decode('utf-8')
                        
                        figure_data = {
                            "figure_id": figure_id,
                            "caption": f"ê·¸ë¦¼ {page_num + 1}-{img_index + 1}",
                            "page_number": page_num + 1,
                            "coordinates": [0.0, 0.0, 100.0, 100.0],  # ê¸°ë³¸ê°’
                            "image_base64": image_base64,
                            "image_path": str(image_path.relative_to(output_dir)),
                            "extracted_text": "",
                            "extraction_tool": "PyMuPDF",
                            "confidence_score": 0.8
                        }
                        
                        figures.append(figure_data)
                        pix = None
                        
                    except Exception as e:
                        self.logger.warning(f"ì´ë¯¸ì§€ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
                        continue
            
            doc.close()
            self.logger.info(f"ì´ {len(figures)}ê°œ ê·¸ë¦¼ ì¶”ì¶œ ì™„ë£Œ")
            return figures
            
        except Exception as e:
            self.logger.error(f"ê·¸ë¦¼ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
            return []
    
    def link_elements(self, text_chunks: List[Dict], tables: List[Dict], figures: List[Dict]) -> List[Dict]:
        """í…ìŠ¤íŠ¸ ì²­í¬ì™€ í‘œ/ê·¸ë¦¼ ê°„ì˜ ì—°ê²° ê³ ë¦¬ ìƒì„±"""
        try:
            self.logger.info("ìš”ì†Œ ê°„ ì—°ê²° ê³ ë¦¬ ìƒì„± ì‹œì‘")
            
            # í‘œ/ê·¸ë¦¼ ID ë§¤í•‘ ìƒì„±
            table_captions = {table["caption"]: table["table_id"] for table in tables if table["caption"]}
            figure_captions = {figure["caption"]: figure["figure_id"] for figure in figures if figure["caption"]}
            
            # í…ìŠ¤íŠ¸ ì²­í¬ ì—…ë°ì´íŠ¸
            for chunk in text_chunks:
                content = chunk["content"].lower()
                
                # í‘œ ê´€ë ¨ì„± í™•ì¸ ë° ì—°ê²°
                if chunk["is_table_related"]:
                    for caption, table_id in table_captions.items():
                        if caption.lower() in content or any(word in content for word in caption.lower().split()):
                            chunk["referenced_element_id"] = table_id
                            break
                
                # ê·¸ë¦¼ ê´€ë ¨ì„± í™•ì¸ ë° ì—°ê²°
                if chunk["is_figure_related"]:
                    for caption, figure_id in figure_captions.items():
                        if caption.lower() in content or any(word in content for word in caption.lower().split()):
                            chunk["referenced_element_id"] = figure_id
                            break
            
            self.logger.info("ìš”ì†Œ ê°„ ì—°ê²° ê³ ë¦¬ ìƒì„± ì™„ë£Œ")
            return text_chunks
            
        except Exception as e:
            self.logger.error(f"ìš”ì†Œ ì—°ê²° ì˜¤ë¥˜: {str(e)}")
            return text_chunks
    
    def create_unified_chunks(self, text_chunks: List[Dict[str, Any]], tables: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """í…ìŠ¤íŠ¸ì™€ í‘œ ì²­í¬ë¥¼ í†µí•©í•˜ì—¬ ë‹¨ì¼ ì²­í¬ ë¦¬ìŠ¤íŠ¸ ìƒì„±"""
        try:
            from utils.table_to_text_converter import TableToTextConverter
            
            self.logger.info("í†µí•© ì²­í¬ ìƒì„± ì‹œì‘")
            
            converter = TableToTextConverter()
            unified_chunks = []
            chunk_counter = 0
            
            # 1. ê¸°ì¡´ í…ìŠ¤íŠ¸ ì²­í¬ ì¶”ê°€ (íƒ€ì… ëª…ì‹œ)
            for text_chunk in text_chunks:
                text_chunk["chunk_index"] = chunk_counter
                unified_chunks.append(text_chunk)
                chunk_counter += 1
            
            # 2. í‘œ ì²­í¬ ìƒì„± ë° ì¶”ê°€
            for table in tables:
                table_chunks = converter.create_table_chunks(table, f"chunk_{chunk_counter:06d}")
                
                for table_chunk in table_chunks:
                    table_chunk["chunk_index"] = chunk_counter
                    unified_chunks.append(table_chunk)
                    chunk_counter += 1
            
            # 3. í˜ì´ì§€ ìˆœì„œëŒ€ë¡œ ì •ë ¬
            unified_chunks.sort(key=lambda x: (x.get('page_number', 1), x.get('chunk_index', 0)))
            
            # 4. ìµœì¢… ID ì¬í• ë‹¹
            for i, chunk in enumerate(unified_chunks):
                chunk["id"] = f"chunk_{i:06d}"
                chunk["chunk_index"] = i
            
            self.logger.info(f"í†µí•© ì²­í¬ ìƒì„± ì™„ë£Œ: í…ìŠ¤íŠ¸ {len(text_chunks)}ê°œ + í‘œ {chunk_counter - len(text_chunks)}ê°œ = ì´ {len(unified_chunks)}ê°œ")
            
            return unified_chunks
            
        except Exception as e:
            self.logger.error(f"í†µí•© ì²­í¬ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            # ê¸°ë³¸ì ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì²­í¬ë§Œ ë°˜í™˜
            return text_chunks

    def process_pdf(self, pdf_path: str, output_dir: str) -> Dict[str, Any]:
        """PDF ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜ - í–¥ìƒëœ í‘œ/í…ìŠ¤íŠ¸ êµ¬ë¶„ ì²˜ë¦¬"""
        try:
            self.logger.info(f"RAG ìµœì í™” PDF ì²˜ë¦¬ ì‹œì‘: {pdf_path}")
            
            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
            output_path = Path(output_dir)
            output_path.mkdir(parents=True, exist_ok=True)
            
            # 1. ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            document_metadata = self.extract_document_metadata(pdf_path)
            
            # 2. í…ìŠ¤íŠ¸ ì²­í¬ ì¶”ì¶œ
            text_chunks = self.extract_text_chunks(pdf_path)
            
            # 3. í‘œ ì¶”ì¶œ
            tables = self.extract_tables(pdf_path)
            
            # 4. ê·¸ë¦¼ ì¶”ì¶œ
            figures = self.extract_figures(pdf_path, output_dir)
            
            # 5. í†µí•© ì²­í¬ ìƒì„± (í…ìŠ¤íŠ¸ + í‘œ ì²­í¬)
            unified_chunks = self.create_unified_chunks(text_chunks, tables)
            
            # 6. ìš”ì†Œ ê°„ ì—°ê²° ê³ ë¦¬ ìƒì„±
            enhanced_chunks = self.link_elements(unified_chunks, tables, figures)
            
            # 7. ì²­í¬ í†µê³„ ê³„ì‚°
            chunk_stats = self._calculate_chunk_statistics(enhanced_chunks)
            
            # 8. ìµœì¢… JSON êµ¬ì¡° ìƒì„±
            result = {
                **document_metadata,
                "text_chunks": enhanced_chunks,
                "tables": tables,
                "figures": figures,
                "chunk_statistics": chunk_stats
            }
            
            # 9. JSON íŒŒì¼ ì €ì¥
            output_filename = f"{Path(pdf_path).stem}_parsed.json"
            output_file = output_path / output_filename
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"RAG ìµœì í™” íŒŒì‹± ì™„ë£Œ: {output_file}")
            
            return {
                "success": True,
                "output_file": str(output_file),
                "text_chunks_count": len([c for c in enhanced_chunks if c.get('type') == 'text']),
                "table_chunks_count": len([c for c in enhanced_chunks if c.get('type') == 'table']),
                "total_chunks_count": len(enhanced_chunks),
                "tables_count": len(tables),
                "figures_count": len(figures),
                "chunk_statistics": chunk_stats
            }
            
        except Exception as e:
            self.logger.error(f"PDF ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def _extract_structured_text_as_tables(self, pdf_path: str) -> List[Dict[str, Any]]:
        """êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ íŒ¨í„´ì„ í‘œë¡œ ë³€í™˜"""
        try:
            self.logger.info("êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ íŒ¨í„´ ê¸°ë°˜ ê°€ìƒ í‘œ ìƒì„± ì‹œì‘")
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text_chunks = self.extract_text_chunks(pdf_path)
            
            virtual_tables = []
            
            # êµ¬ì¡°í™”ëœ íŒ¨í„´ ì°¾ê¸°
            structured_patterns = ['â‘ ', 'â‘¡', 'â‘¢', 'â‘£', 'â‘¤', 'ê°€)', 'ë‚˜)', 'ë‹¤)', 'ë¼)', 'ë§ˆ)']
            current_table_items = []
            current_page = 1
            table_counter = 1
            
            for chunk in text_chunks:
                text = chunk.get('text', '')
                page_num = chunk.get('page_number', current_page)
                
                # êµ¬ì¡°í™”ëœ í•­ëª©ì¸ì§€ í™•ì¸
                has_pattern = any(pattern in text for pattern in structured_patterns)
                
                if has_pattern:
                    # ì—¬ëŸ¬ íŒ¨í„´ì´ í•œ í…ìŠ¤íŠ¸ì— ìˆëŠ” ê²½ìš° ë¶„ë¦¬
                    lines = text.split('\n')
                    for line in lines:
                        line = line.strip()
                        if any(pattern in line for pattern in structured_patterns):
                            current_table_items.append({
                                'text': line,
                                'page': page_num
                            })
                
                # í˜ì´ì§€ê°€ ë°”ë€Œê±°ë‚˜ ì¶©ë¶„í•œ í•­ëª©ì´ ëª¨ì˜€ì„ ë•Œ ê°€ìƒ í‘œ ìƒì„±
                if (page_num != current_page and current_table_items) or len(current_table_items) >= 5:
                    if len(current_table_items) >= 2:  # ìµœì†Œ 2ê°œ í•­ëª©ë¶€í„° í‘œë¡œ ê°„ì£¼
                        virtual_table = self._create_virtual_table(
                            current_table_items, 
                            current_page, 
                            table_counter
                        )
                        virtual_tables.append(virtual_table)
                        table_counter += 1
                    
                    current_table_items = []
                    current_page = page_num
            
            # ë§ˆì§€ë§‰ ë‚¨ì€ í•­ëª©ë“¤ ì²˜ë¦¬
            if len(current_table_items) >= 2:
                virtual_table = self._create_virtual_table(
                    current_table_items, 
                    current_page, 
                    table_counter
                )
                virtual_tables.append(virtual_table)
            
            self.logger.info(f"êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ë¡œë¶€í„° {len(virtual_tables)}ê°œ ê°€ìƒ í‘œ ìƒì„±")
            return virtual_tables
            
        except Exception as e:
            self.logger.error(f"êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ í‘œ ë³€í™˜ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def _create_virtual_table(self, items: List[Dict], page_num: int, table_id: int) -> Dict[str, Any]:
        """êµ¬ì¡°í™”ëœ í•­ëª©ë“¤ë¡œë¶€í„° ê°€ìƒ í‘œ ìƒì„±"""
        try:
            # í‘œ ë°ì´í„° êµ¬ì„±
            table_data = []
            cell_texts = []
            
            for i, item in enumerate(items):
                # ê° í•­ëª©ì„ í–‰ìœ¼ë¡œ ë³€í™˜
                text = item['text']
                
                # íŒ¨í„´ê³¼ ë‚´ìš© ë¶„ë¦¬
                for pattern in ['â‘ ', 'â‘¡', 'â‘¢', 'â‘£', 'â‘¤', 'ê°€)', 'ë‚˜)', 'ë‹¤)', 'ë¼)', 'ë§ˆ)']:
                    if pattern in text:
                        parts = text.split(pattern, 1)
                        if len(parts) == 2:
                            key = pattern.strip()
                            value = parts[1].strip()
                            table_data.append([key, value])
                            
                            # ì…€ í…ìŠ¤íŠ¸ ì •ë³´
                            cell_texts.extend([
                                {"text": key, "row": i, "col": 0},
                                {"text": value, "row": i, "col": 1}
                            ])
                        break
                else:
                    # íŒ¨í„´ì´ ì—†ëŠ” ê²½ìš° ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì¼ ì…€ë¡œ
                    table_data.append([text])
                    cell_texts.append({"text": text, "row": i, "col": 0})
            
            # ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ìƒì„±
            extracted_text = " ".join([item['text'] for item in items])
            
            # ê°€ìƒ í‘œ ë©”íƒ€ë°ì´í„°
            virtual_table = {
                "table_id": f"virtual_table_{table_id:03d}",
                "caption": f"êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ í‘œ {table_id} (íŒ¨í„´ ê¸°ë°˜)",
                "page_number": page_num,
                "coordinates": [0, 0, 100, 100],  # ê°€ìƒ ì¢Œí‘œ
                "structure": {
                    "rows": len(table_data),
                    "columns": max(len(row) for row in table_data) if table_data else 0
                },
                "table_data": table_data,
                "extracted_text": extracted_text,
                "extraction_tool": "Pattern_Based_Virtual_Table",
                "confidence_score": 0.8,  # íŒ¨í„´ ê¸°ë°˜ì´ë¯€ë¡œ ë†’ì€ ì‹ ë¢°ë„
                "cell_texts": cell_texts,
                "virtual_table": True,  # ê°€ìƒ í‘œì„ì„ í‘œì‹œ
                "source_items": len(items)
            }
            
            return virtual_table
            
        except Exception as e:
            self.logger.error(f"ê°€ìƒ í‘œ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return {}

    def _calculate_chunk_statistics(self, chunks: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ì²­í¬ í†µê³„ ê³„ì‚° - íƒ€ì…ë³„ ìƒì„¸ ë¶„ì„"""
        try:
            # íƒ€ì…ë³„ ë¶„ë¥˜
            text_chunks = [c for c in chunks if c.get('type') == 'text']
            table_chunks = [c for c in chunks if c.get('type') == 'table']
            
            # ê¸°ë³¸ í†µê³„
            stats = {
                "total_chunks": len(chunks),
                "text_chunks_count": len(text_chunks),
                "table_chunks_count": len(table_chunks),
                "text_ratio": round(len(text_chunks) / len(chunks) if chunks else 0, 3),
                "table_ratio": round(len(table_chunks) / len(chunks) if chunks else 0, 3),
                "avg_text_length": round(sum(c.get('word_count', 0) for c in text_chunks) / len(text_chunks) if text_chunks else 0, 1),
                "avg_table_length": round(sum(c.get('word_count', 0) for c in table_chunks) / len(table_chunks) if table_chunks else 0, 1),
                "total_words": sum(c.get('word_count', 0) for c in chunks),
                "total_tokens_estimate": sum(c.get('token_estimate', 0) for c in chunks)
            }
            
            # íƒ€ì…ë³„ ë¶„í¬
            stats["chunk_type_distribution"] = {
                "text": len(text_chunks),
                "table": len(table_chunks)
            }
            
            # í…ìŠ¤íŠ¸ ì²­í¬ ì„œë¸Œíƒ€ì…
            if text_chunks:
                text_subtypes = {}
                for chunk in text_chunks:
                    subtype = chunk.get('subtype', 'unknown')
                    text_subtypes[subtype] = text_subtypes.get(subtype, 0) + 1
                stats["text_chunk_subtypes"] = text_subtypes
            
            # í‘œ ì²­í¬ ì„œë¸Œíƒ€ì…
            if table_chunks:
                table_subtypes = {}
                for chunk in table_chunks:
                    subtype = chunk.get('subtype', 'unknown')
                    table_subtypes[subtype] = table_subtypes.get(subtype, 0) + 1
                stats["table_chunk_subtypes"] = table_subtypes
            
            # ì¶”ì¶œ ë„êµ¬ë³„ í†µê³„
            extraction_tools = {}
            for chunk in chunks:
                tool = chunk.get('extraction_tool', 'unknown')
                extraction_tools[tool] = extraction_tools.get(tool, 0) + 1
            stats["extraction_tools_used"] = extraction_tools
            
            # í‘œ ì‹ ë¢°ë„ í†µê³„
            table_confidences = [c.get('confidence_score', 0) for c in table_chunks if 'confidence_score' in c]
            if table_confidences:
                stats["table_confidence_stats"] = {
                    "average": round(sum(table_confidences) / len(table_confidences), 3),
                    "min": round(min(table_confidences), 3),
                    "max": round(max(table_confidences), 3),
                    "count": len(table_confidences)
                }
            
            return stats
            
        except Exception as e:
            self.logger.error(f"ì²­í¬ í†µê³„ ê³„ì‚° ì‹¤íŒ¨: {str(e)}")
            return {
                "total_chunks": len(chunks),
                "text_chunks_count": 0,
                "table_chunks_count": 0,
                "error": str(e)
            }
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            import shutil
            if self.temp_dir.exists():
                shutil.rmtree(self.temp_dir)
            self.logger.info("ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            self.logger.error(f"ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì˜¤ë¥˜: {str(e)}")
    
    def __del__(self):
        """ì†Œë©¸ì"""
        self.cleanup() 