#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
í‚¤ì›Œë“œ ì¶”ì¶œê¸°
JSON íŒŒì¼ì˜ ê° ì²­í¬ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì—¬ ìƒˆë¡œìš´ í•„ë“œë¡œ ì¶”ê°€
ë¬¸ì„œ ì œëª© ê¸°ì¤€ ìœ ì‚¬ë„ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
"""

import json
import logging
from pathlib import Path
from typing import List, Dict, Any
import re
from collections import Counter

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class KeywordExtractor:
    """JSON ì²­í¬ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):
        """
        í‚¤ì›Œë“œ ì¶”ì¶œê¸° ì´ˆê¸°í™”
        
        Args:
            model_name: ì‚¬ìš©í•  sentence transformer ëª¨ë¸ëª…
        """
        self.logger = logging.getLogger(__name__)
        self.logger.info(f"KeyBERT ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
        
        try:
            # Sentence Transformer ëª¨ë¸ ë¡œë“œ
            from sentence_transformers import SentenceTransformer
            sentence_model = SentenceTransformer(model_name)
            
            # KeyBERT ì´ˆê¸°í™”
            from keybert import KeyBERT
            self.keybert = KeyBERT(model=sentence_model)
            self.logger.info("âœ… KeyBERT ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            
        except ImportError as e:
            self.logger.error(f"âŒ KeyBERT ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
            self.keybert = None
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            self.keybert = None
    
    def clean_text(self, text: str) -> str:
        """
        í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        
        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸
            
        Returns:
            ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸
        """
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê³µë°±ë§Œ ìœ ì§€)
        cleaned = re.sub(r'[^\w\sê°€-í£]', ' ', text)
        # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        cleaned = re.sub(r'\s+', ' ', cleaned)
        return cleaned.strip()
    
    def extract_keywords_with_title_similarity(self, text: str, document_title: str, top_k: int = 5) -> List[str]:
        """
        ë¬¸ì„œ ì œëª© ê¸°ì¤€ ìœ ì‚¬ë„ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ (ë°˜ë“œì‹œ 5ê°œ í‚¤ì›Œë“œ ë³´ì¥)
        
        Args:
            text: í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•  í…ìŠ¤íŠ¸
            document_title: ë¬¸ì„œ ì œëª© (ìœ ì‚¬ë„ ê³„ì‚° ê¸°ì¤€)
            top_k: ì¶”ì¶œí•  í‚¤ì›Œë“œ ê°œìˆ˜ (ê¸°ë³¸ê°’: 5)
            
        Returns:
            ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ë°˜ë“œì‹œ top_kê°œ)
        """
        try:
            if self.keybert is None:
                # KeyBERTê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ì¶œ ì‚¬ìš©
                return self.extract_fallback_keywords(text, top_k)
            
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            cleaned_text = self.clean_text(text)
            
            if len(cleaned_text) < 10:  # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ëŠ” ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ì¶œ
                return self.extract_fallback_keywords(text, top_k)
            
            # 1ë‹¨ê³„: ë¬¸ì„œ ì œëª©ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
            title_keywords = self.extract_title_keywords(document_title)
            
            # 2ë‹¨ê³„: KeyBERTë¡œ í‚¤ì›Œë“œ í›„ë³´ ì¶”ì¶œ (ë” ë§ì€ í›„ë³´ ìƒì„±)
            keyword_candidates = self.keybert.extract_keywords(
                cleaned_text,
                keyphrase_ngram_range=(1, 3),  # 1-3ë‹¨ì–´ ì¡°í•©
                stop_words=None,
                top_k=50,  # ë” ë§ì€ í›„ë³´ ìƒì„±
                diversity=0.9  # ë‹¤ì–‘ì„± ë³´ì¥
            )
            
            if not keyword_candidates:
                return self.extract_fallback_keywords(text, top_k)
            
            # 3ë‹¨ê³„: ë¬¸ì„œ ì œëª© í‚¤ì›Œë“œì™€ì˜ ìœ ì‚¬ë„ ê³„ì‚° ë° ìš°ì„ ìˆœìœ„ ë¶€ì—¬
            title_similarities = []
            for keyword, score in keyword_candidates:
                # í‚¤ì›Œë“œì™€ ë¬¸ì„œ ì œëª©ì˜ ìœ ì‚¬ë„ ê³„ì‚°
                similarity = self.calculate_title_similarity(keyword, document_title)
                
                # ë¬¸ì„œ ì œëª© í‚¤ì›Œë“œì™€ì˜ ì§ì ‘ ë§¤ì¹­ ì ìˆ˜ ì¶”ê°€
                title_match_score = self.calculate_title_keyword_match(keyword, title_keywords)
                
                # ìµœì¢… ì ìˆ˜: ìœ ì‚¬ë„ + ì œëª© í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
                final_score = similarity + (title_match_score * 0.5)  # ì œëª© í‚¤ì›Œë“œ ë§¤ì¹­ì— ê°€ì¤‘ì¹˜
                
                title_similarities.append((keyword, final_score))
            
            # 4ë‹¨ê³„: ìµœì¢… ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            title_similarities.sort(key=lambda x: x[1], reverse=True)
            
            # 5ë‹¨ê³„: ë°˜ë“œì‹œ top_kê°œ í‚¤ì›Œë“œ ë³´ì¥
            selected_keywords = []
            
            # ìš°ì„ ìˆœìœ„ê°€ ë†’ì€ í‚¤ì›Œë“œë“¤ ë¨¼ì € ì„ íƒ
            for keyword, score in title_similarities:
                if len(selected_keywords) >= top_k:
                    break
                if keyword not in selected_keywords:
                    selected_keywords.append(keyword)
            
            # ë§Œì•½ top_kê°œê°€ ì•ˆ ë˜ë©´ ì¶”ê°€ í‚¤ì›Œë“œ ì¶”ì¶œ
            if len(selected_keywords) < top_k:
                additional_keywords = self.extract_additional_keywords(text, selected_keywords, top_k - len(selected_keywords))
                selected_keywords.extend(additional_keywords)
            
            # ìµœì¢…ì ìœ¼ë¡œ top_kê°œë§Œ ë°˜í™˜
            return selected_keywords[:top_k]
            
        except Exception as e:
            self.logger.error(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return self.extract_fallback_keywords(text, top_k)
    
    def extract_title_keywords(self, document_title: str) -> List[str]:
        """
        ë¬¸ì„œ ì œëª©ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        
        Args:
            document_title: ë¬¸ì„œ ì œëª©
            
        Returns:
            ì œëª©ì—ì„œ ì¶”ì¶œëœ í•µì‹¬ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        """
        try:
            if self.keybert is None:
                # KeyBERTê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ì¶œ
                return self.extract_fallback_keywords(document_title, 10)
            
            # ë¬¸ì„œ ì œëª©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
            title_keywords = self.keybert.extract_keywords(
                document_title,
                keyphrase_ngram_range=(1, 3),
                stop_words=None,
                top_k=10,
                diversity=0.8
            )
            
            # í‚¤ì›Œë“œë§Œ ì¶”ì¶œ
            keywords = [keyword for keyword, score in title_keywords]
            return keywords
            
        except Exception as e:
            self.logger.warning(f"ì œëª© í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def calculate_title_keyword_match(self, keyword: str, title_keywords: List[str]) -> float:
        """
        í‚¤ì›Œë“œì™€ ì œëª© í‚¤ì›Œë“œ ê°„ì˜ ì§ì ‘ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
        
        Args:
            keyword: í‚¤ì›Œë“œ
            title_keywords: ì œëª©ì—ì„œ ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ë§¤ì¹­ ì ìˆ˜ (0.0 ~ 1.0)
        """
        if not title_keywords:
            return 0.0
        
        # ì •í™•í•œ ë§¤ì¹­
        if keyword in title_keywords:
            return 1.0
        
        # ë¶€ë¶„ ë§¤ì¹­ (í‚¤ì›Œë“œê°€ ì œëª© í‚¤ì›Œë“œì— í¬í•¨ë˜ëŠ” ê²½ìš°)
        keyword_lower = keyword.lower()
        for title_keyword in title_keywords:
            title_keyword_lower = title_keyword.lower()
            
            # í‚¤ì›Œë“œê°€ ì œëª© í‚¤ì›Œë“œì— í¬í•¨ë˜ê±°ë‚˜, ì œëª© í‚¤ì›Œë“œê°€ í‚¤ì›Œë“œì— í¬í•¨ë˜ëŠ” ê²½ìš°
            if keyword_lower in title_keyword_lower or title_keyword_lower in keyword_lower:
                return 0.8
        
        # ë‹¨ì–´ ë‹¨ìœ„ ë§¤ì¹­
        keyword_words = set(keyword_lower.split())
        for title_keyword in title_keywords:
            title_keyword_words = set(title_keyword.lower().split())
            
            # ê²¹ì¹˜ëŠ” ë‹¨ì–´ê°€ ìˆëŠ” ê²½ìš°
            intersection = keyword_words.intersection(title_keyword_words)
            if intersection:
                return 0.6 * (len(intersection) / max(len(keyword_words), len(title_keyword_words)))
        
        return 0.0
    
    def calculate_title_similarity(self, keyword: str, document_title: str) -> float:
        """
        í‚¤ì›Œë“œì™€ ë¬¸ì„œ ì œëª© ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°
        
        Args:
            keyword: í‚¤ì›Œë“œ
            document_title: ë¬¸ì„œ ì œëª©
            
        Returns:
            ìœ ì‚¬ë„ ì ìˆ˜ (0.0 ~ 1.0)
        """
        try:
            if self.keybert is None:
                return self.calculate_simple_similarity(keyword, document_title)
            
            # KeyBERTë¥¼ ì‚¬ìš©í•œ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°
            embeddings = self.keybert.model.encode([keyword, document_title])
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            from sklearn.metrics.pairwise import cosine_similarity
            similarity_matrix = cosine_similarity([embeddings[0]], [embeddings[1]])
            similarity_score = similarity_matrix[0][0]
            
            return float(similarity_score)
            
        except Exception as e:
            self.logger.warning(f"ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨, ê¸°ë³¸ ë°©ë²• ì‚¬ìš©: {e}")
            return self.calculate_simple_similarity(keyword, document_title)
    
    def calculate_simple_similarity(self, keyword: str, document_title: str) -> float:
        """
        ê°„ë‹¨í•œ ìœ ì‚¬ë„ ê³„ì‚° (ë‹¨ì–´ ê²¹ì¹¨ ê¸°ë°˜)
        
        Args:
            keyword: í‚¤ì›Œë“œ
            document_title: ë¬¸ì„œ ì œëª©
            
        Returns:
            ìœ ì‚¬ë„ ì ìˆ˜ (0.0 ~ 1.0)
        """
        # í…ìŠ¤íŠ¸ ì •ë¦¬
        keyword_clean = self.clean_text(keyword.lower())
        title_clean = self.clean_text(document_title.lower())
        
        # ë‹¨ì–´ ë¶„ë¦¬
        keyword_words = set(keyword_clean.split())
        title_words = set(title_clean.split())
        
        if not keyword_words or not title_words:
            return 0.0
        
        # ê²¹ì¹˜ëŠ” ë‹¨ì–´ ìˆ˜ ê³„ì‚°
        intersection = keyword_words.intersection(title_words)
        union = keyword_words.union(title_words)
        
        # Jaccard ìœ ì‚¬ë„
        similarity = len(intersection) / len(union) if union else 0.0
        
        return similarity
    
    def extract_fallback_keywords(self, text: str, top_k: int = 5) -> List[str]:
        """
        KeyBERT ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•  ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ì¶œ (ë°˜ë“œì‹œ top_kê°œ ë³´ì¥)
        
        Args:
            text: í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•  í…ìŠ¤íŠ¸
            top_k: ì¶”ì¶œí•  í‚¤ì›Œë“œ ê°œìˆ˜
            
        Returns:
            ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ë°˜ë“œì‹œ top_kê°œ)
        """
        # ì˜ë£Œ ê´€ë ¨ í‚¤ì›Œë“œ íŒ¨í„´
        medical_keywords = self.get_medical_keywords()
        
        # í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì°¾ê¸°
        found_keywords = []
        for keyword in medical_keywords:
            if keyword in text:
                found_keywords.append(keyword)
        
        # ë§Œì•½ ì˜ë£Œ í‚¤ì›Œë“œê°€ ë¶€ì¡±í•˜ë©´ ì¶”ê°€ í‚¤ì›Œë“œ ì¶”ì¶œ
        if len(found_keywords) < top_k:
            additional_keywords = self.extract_additional_keywords(text, found_keywords, top_k - len(found_keywords))
            found_keywords.extend(additional_keywords)
        
        # ìµœì¢…ì ìœ¼ë¡œ top_kê°œë§Œ ë°˜í™˜
        return found_keywords[:top_k]
    
    def extract_additional_keywords(self, text: str, existing_keywords: List[str], needed_count: int) -> List[str]:
        """
        ì¶”ê°€ í‚¤ì›Œë“œ ì¶”ì¶œ (ì˜ë¯¸ ìˆëŠ” ë‹¨ì–´ë“¤)
        
        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸
            existing_keywords: ì´ë¯¸ ì„ íƒëœ í‚¤ì›Œë“œë“¤
            needed_count: ì¶”ê°€ë¡œ í•„ìš”í•œ í‚¤ì›Œë“œ ê°œìˆ˜
            
        Returns:
            ì¶”ê°€ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            cleaned_text = self.clean_text(text)
            words = cleaned_text.split()
            
            # 2ê¸€ì ì´ìƒì˜ ì˜ë¯¸ ìˆëŠ” ë‹¨ì–´ë“¤ í•„í„°ë§
            meaningful_words = []
            for word in words:
                if len(word) >= 2 and word not in existing_keywords:
                    # ë¶ˆìš©ì–´ ì œê±°
                    if word not in self.get_stop_words():
                        meaningful_words.append(word)
            
            # ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
            word_freq = Counter(meaningful_words)
            
            # ë¹ˆë„ìˆœìœ¼ë¡œ ì •ë ¬
            sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
            
            # ì¶”ê°€ í‚¤ì›Œë“œ ì„ íƒ
            additional_keywords = []
            for word, freq in sorted_words:
                if len(additional_keywords) >= needed_count:
                    break
                if word not in existing_keywords and word not in additional_keywords:
                    additional_keywords.append(word)
            
            # ì˜ë£Œ ê´€ë ¨ í‚¤ì›Œë“œ ì‚¬ì „ì—ì„œë„ ì¶”ê°€
            if len(additional_keywords) < needed_count:
                medical_keywords = self.get_medical_keywords()
                for keyword in medical_keywords:
                    if len(additional_keywords) >= needed_count:
                        break
                    if keyword in text and keyword not in existing_keywords and keyword not in additional_keywords:
                        additional_keywords.append(keyword)
            
            return additional_keywords
            
        except Exception as e:
            self.logger.warning(f"ì¶”ê°€ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def get_stop_words(self) -> List[str]:
        """í•œêµ­ì–´ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸"""
        return [
            'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ë¡œ', 'ìœ¼ë¡œ', 'ì™€', 'ê³¼', 'ë„', 'ë§Œ', 'ì€', 'ëŠ”',
            'ê·¸', 'ì´', 'ì €', 'ìš°ë¦¬', 'ê·¸ë“¤', 'ì´ë“¤', 'ì €ë“¤', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë°', 'ë˜ëŠ”',
            'ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ë˜í•œ', 'ë˜í•œ', 'ë˜í•œ', 'ë˜í•œ', 'ë˜í•œ', 'ë˜í•œ',
            'ìˆ', 'í•˜', 'ë˜', 'ê²ƒ', 'ë“¤', 'ìˆ˜', 'ì´', 'ë³´', 'ì•Š', 'ì—†', 'ë‚˜', 'ì‚¬ëŒ', 'ì£¼', 'ì•„ë‹ˆ', 'ë“±', 'ê°™', 'ìš°ë¦¬', 'ë•Œ', 'ë…„', 'ê°€', 'í•œ', 'ì§€', 'ëŒ€í•˜', 'ì˜¤', 'ë§', 'ì¼', 'ê·¸ë ‡', 'ìœ„í•˜'
        ]
    
    def get_medical_keywords(self) -> List[str]:
        """ì˜ë£Œ ê´€ë ¨ í‚¤ì›Œë“œ ì‚¬ì „"""
        return [
            'PSUR', 'ì•ˆì „ì„±', 'ì •ë³´', 'ë³´ê³ ', 'í—ˆê°€', 'í‰ê°€', 'ì˜ë£Œê¸°ê¸°', 'ì˜ì•½í’ˆ',
            'í’ˆì§ˆ', 'ê´€ë¦¬', 'ì‹œì •', 'ì˜ˆë°©', 'ì¡°ì¹˜', 'ì‹¬ì‚¬', 'ì ˆì°¨', 'ì œì¡°ì—…ì²´',
            'ì‹ì•½ì²˜', 'ê°€ì´ë“œë¼ì¸', 'ì„œë¥˜', 'ì²¨ë‹¨ë°”ì´ì˜¤ì˜ì•½í’ˆ', 'ì‹œíŒ', 'í’ˆì§ˆí‰ê°€',
            'ìƒë¬¼í•™ì ', 'íŠ¹ì„±', 'ì œì¡°', 'ê³µì •', 'ë³µì¡ì„±', 'GMP', 'ì‹œìŠ¤í…œ',
            'ê°œì„ ', 'ìœ„í—˜', 'ë°©ì§€', 'ê¸°ìˆ ë¬¸ì„œ', 'ì„ìƒë°ì´í„°', 'ìœ íš¨ì„±', 'ì‹¬ì‚¬ê¸°ê´€',
            'í”„ë¡œíŒŒì¼', 'ëª¨ë‹ˆí„°ë§', 'ë„êµ¬', 'ì¸ì¦', 'ì‹ ê³ ', 'ê³ ë ¤ì‚¬í•­', 'ë°©ë²•',
            'ì ‘ê·¼', 'ê³¼ì •', 'ì…ì¦', 'ìš”êµ¬ì‚¬í•­', 'ìë£Œ', 'ë³´ì™„', 'ê²€ì¦', 'ìŠ¹ì¸',
            'ë“±ë¡', 'ë³€ê²½', 'íì§€', 'ì·¨ì†Œ', 'ì •ì§€', 'ì œí•œ', 'ì¡°ê±´', 'ê¸°ê°„',
            'ë²”ìœ„', 'ëŒ€ìƒ', 'ê¸°ì¤€', 'ê·œì •', 'ì§€ì¹¨', 'ë§¤ë‰´ì–¼', 'ì ˆì°¨ì„œ', 'í‘œì¤€',
            'ê·œê²©', 'ì‚¬ì–‘', 'ì„±ëŠ¥', 'ê¸°ëŠ¥', 'íš¨ê³¼', 'ê²°ê³¼', 'ë¶„ì„', 'ê²€í† ',
            'ê²€ì‚¬', 'ì‹œí—˜', 'ì¸¡ì •', 'ê³„ì‚°', 'ì‚°ì¶œ', 'ë„ì¶œ', 'ê²°ì •', 'íŒë‹¨',
            'ì˜ê²¬', 'ì œì•ˆ', 'ê¶Œê³ ', 'ì§€ì‹œ', 'ëª…ë ¹', 'ìš”ì²­', 'ì‹ ì²­', 'ì œì¶œ',
            'ì ‘ìˆ˜', 'ì²˜ë¦¬', 'ê²€í† ', 'ì‹¬ì˜', 'ì˜ê²°', 'ê²°ì •', 'í†µë³´', 'ê³ ì§€',
            'ê³µê³ ', 'ë°œí‘œ', 'ê³µê°œ', 'ì œê³µ', 'êµë¶€', 'ìˆ˜ë ¹', 'ì ‘ìˆ˜', 'ì²˜ë¦¬'
        ]
    
    def extract_document_title(self, text_chunks: List[Dict[str, Any]]) -> str:
        """
        í…ìŠ¤íŠ¸ ì²­í¬ì—ì„œ ë¬¸ì„œ ì œëª© ì¶”ì¶œ
        
        Args:
            text_chunks: í…ìŠ¤íŠ¸ ì²­í¬ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ì¶”ì¶œëœ ë¬¸ì„œ ì œëª©
        """
        # ì²« ë²ˆì§¸ ì²­í¬ì—ì„œ ì œëª© ì¶”ì¶œ ì‹œë„
        if text_chunks and len(text_chunks) > 0:
            first_chunk = text_chunks[0]
            
            # metadataì—ì„œ heading í™•ì¸
            if 'metadata' in first_chunk and 'heading' in first_chunk['metadata']:
                heading = first_chunk['metadata']['heading']
                if heading and len(heading.strip()) > 0:
                    return heading.strip()
            
            # textì—ì„œ ì²« ë²ˆì§¸ ì¤„ì„ ì œëª©ìœ¼ë¡œ ì‚¬ìš©
            if 'text' in first_chunk:
                text = first_chunk['text']
                lines = text.split('\n')
                for line in lines:
                    line = line.strip()
                    if line and len(line) > 5 and len(line) < 100:
                        return line
        
        return "ì˜ë£Œê¸°ê¸° ê°€ì´ë“œë¼ì¸"
    
    def process_json_file(self, input_file: str, output_file: str = None) -> str:
        """
        JSON íŒŒì¼ì˜ ëª¨ë“  ì²­í¬ì— í‚¤ì›Œë“œ ì¶”ê°€
        
        Args:
            input_file: ì…ë ¥ JSON íŒŒì¼ ê²½ë¡œ
            output_file: ì¶œë ¥ JSON íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ìë™ ìƒì„±)
            
        Returns:
            ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
        """
        input_path = Path(input_file)
        
        if not input_path.exists():
            raise FileNotFoundError(f"ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_file}")
        
        # ì¶œë ¥ íŒŒì¼ëª… ì„¤ì •
        if output_file is None:
            output_path = input_path.parent / f"{input_path.stem}_with_keywords.json"
        else:
            output_path = Path(output_file)
        
        self.logger.info(f"ğŸ“– JSON íŒŒì¼ ë¡œë”©: {input_file}")
        
        try:
            # JSON íŒŒì¼ ë¡œë“œ
            with open(input_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if not isinstance(data, list):
                raise ValueError("JSON íŒŒì¼ì€ ë°°ì—´ í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤.")
            
            self.logger.info(f"ğŸ“Š ì´ {len(data)}ê°œ ì²­í¬ ì²˜ë¦¬ ì‹œì‘")
            
            # ë¬¸ì„œ ì œëª© ì¶”ì¶œ
            document_title = self.extract_document_title(data)
            self.logger.info(f"ğŸ“‹ ë¬¸ì„œ ì œëª©: {document_title}")
            
            # ê° ì²­í¬ì— í‚¤ì›Œë“œ ì¶”ê°€
            processed_data = []
            for i, chunk in enumerate(data):
                if i % 10 == 0:  # ì§„í–‰ìƒí™© ë¡œê·¸
                    self.logger.info(f"ì²˜ë¦¬ ì¤‘... {i+1}/{len(data)}")
                
                # ì²­í¬ ë³µì‚¬
                processed_chunk = chunk.copy()
                
                # text í•„ë“œ í™•ì¸
                if 'text' not in processed_chunk:
                    self.logger.warning(f"ì²­í¬ {i}ì— 'text' í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
                    processed_chunk['keywords'] = []
                    processed_data.append(processed_chunk)
                    continue
                
                # í‚¤ì›Œë“œ ì¶”ì¶œ (ë¬¸ì„œ ì œëª© ê¸°ì¤€ ìœ ì‚¬ë„)
                text = processed_chunk['text']
                keywords = self.extract_keywords_with_title_similarity(text, document_title)
                
                # í‚¤ì›Œë“œ í•„ë“œ ì¶”ê°€
                processed_chunk['keywords'] = keywords
                processed_data.append(processed_chunk)
            
            # ê²°ê³¼ ì €ì¥
            self.logger.info(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(processed_data, f, ensure_ascii=False, indent=2)
            
            self.logger.info("âœ… í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ!")
            return str(output_path)
            
        except Exception as e:
            self.logger.error(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='JSON ì²­í¬ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ')
    parser.add_argument('--input', '-i', required=True, help='ì…ë ¥ JSON íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--output', '-o', help='ì¶œë ¥ JSON íŒŒì¼ ê²½ë¡œ (ì„ íƒì‚¬í•­)')
    parser.add_argument('--model', '-m', default='all-MiniLM-L6-v2', help='ì‚¬ìš©í•  ëª¨ë¸ëª…')
    parser.add_argument('--top-k', '-k', type=int, default=5, help='ì¶”ì¶œí•  í‚¤ì›Œë“œ ê°œìˆ˜')
    
    args = parser.parse_args()
    
    try:
        # í‚¤ì›Œë“œ ì¶”ì¶œê¸° ì´ˆê¸°í™”
        extractor = KeywordExtractor(model_name=args.model)
        
        # íŒŒì¼ ì²˜ë¦¬
        output_file = extractor.process_json_file(args.input, args.output)
        
        print(f"\nğŸ‰ í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ!")
        print(f"ğŸ“ ì¶œë ¥ íŒŒì¼: {output_file}")
        
    except Exception as e:
        logger.error(f"ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main()) 