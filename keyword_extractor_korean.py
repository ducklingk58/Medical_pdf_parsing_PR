#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
í•œê¸€ ë¶ˆìš©ì–´ ì œê±° KeyBERT í‚¤ì›Œë“œ ì¶”ì¶œê¸°
JSON íŒŒì¼ì˜ ê° ì²­í¬ì—ì„œ í•œê¸€ ë¶ˆìš©ì–´ë¥¼ ì œê±°í•˜ê³  KeyBERTë¡œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œ
"""

import json
import logging
from pathlib import Path
from typing import List, Dict, Any
import re
from collections import Counter

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class KoreanKeywordExtractor:
    """í•œê¸€ ë¶ˆìš©ì–´ ì œê±° KeyBERT í‚¤ì›Œë“œ ì¶”ì¶œê¸°"""
    
    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):
        """
        í‚¤ì›Œë“œ ì¶”ì¶œê¸° ì´ˆê¸°í™”
        
        Args:
            model_name: ì‚¬ìš©í•  sentence transformer ëª¨ë¸ëª…
        """
        self.logger = logging.getLogger(__name__)
        self.logger.info(f"KeyBERT ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
        
        try:
            # Sentence Transformer ëª¨ë¸ ë¡œë“œ
            from sentence_transformers import SentenceTransformer
            sentence_model = SentenceTransformer(model_name)
            
            # KeyBERT ì´ˆê¸°í™”
            from keybert import KeyBERT
            self.keybert = KeyBERT(model=sentence_model)
            self.logger.info("âœ… KeyBERT ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            
        except ImportError as e:
            self.logger.error(f"âŒ KeyBERT ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
            self.keybert = None
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            self.keybert = None
    
    def get_korean_stopwords(self) -> set:
        """
        í•œê¸€ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        
        Returns:
            í•œê¸€ ë¶ˆìš©ì–´ ì§‘í•©
        """
        return {
            "ì •ë³´", "ë‚´ìš©", "ì‚¬í•­", "ê¸°ì¤€", "ì„¤ì •", "ì‚¬ìš©", "ì ìš©", "ëŒ€ìƒ", "ìš”êµ¬", "ê´€ë¦¬", "ì •ì˜",
            "ì ˆì°¨", "ì œê³µ", "ì¤‘ìš”", "í•„ìˆ˜", "ì¡°ì¹˜", "ë°©ë²•", "í˜•íƒœ", "ì‘ì„±", "í•„ìš”", "ê²½ìš°", "í™•ì¸",
            "ê´€ë ¨", "ì œí’ˆ", "êµ¬ì„±", "ì„±ë¶„", "ë¬¸ì„œ", "í•­ëª©", "ì‹œí–‰", "ê²€í† ", "ìˆ˜í–‰", "ì œì¶œ", "ì˜ˆì‹œ",
            "ì„¤ëª…", "ì§„í–‰", "ì´ìš©", "ëª©ì ", "ìš”ì•½", "ë‚´ìš©", "ê²€ì‚¬", "ì¡°ê±´", "í•œê³„", "í˜•ì‹", "ê¸°ì¬",
            "ê·¼ê±°", "í•­ëª©", "ìë£Œ", "ì‹¤ì‹œ", "ìœ í˜•", "ì¢…ë¥˜", "ë°©ë²•", "ì ˆì°¨", "ì‹œê¸°", "í˜•íƒœ", "ëŒ€ì‘",
            "ìœ ë¬´", "ë‹¨ê³„", "í¬í•¨", "ì´ìœ ", "í‘œê¸°", "ìœ ì§€", "ê¸°ê°„", "êµ¬ë¶„", "ì •ë„", "íŠ¹ì„±", "í™œìš©",
            # ì¶”ê°€ í•œê¸€ ë¶ˆìš©ì–´
            "ì´", "ê°€", "ì„", "ë¥¼", "ì˜", "ì—", "ë¡œ", "ìœ¼ë¡œ", "ì™€", "ê³¼", "ë„", "ë§Œ", "ì€", "ëŠ”",
            "ê·¸", "ì´", "ì €", "ìš°ë¦¬", "ê·¸ë“¤", "ì´ë“¤", "ì €ë“¤", "ê²ƒ", "ìˆ˜", "ë“±", "ë°", "ë˜ëŠ”",
            "ê·¸ë¦¬ê³ ", "í•˜ì§€ë§Œ", "ê·¸ëŸ¬ë‚˜", "ë˜í•œ", "ìˆ", "í•˜", "ë˜", "ë“¤", "ë³´", "ì•Š", "ì—†", "ë‚˜",
            "ì‚¬ëŒ", "ì£¼", "ì•„ë‹ˆ", "ê°™", "ë•Œ", "ë…„", "í•œ", "ì§€", "ëŒ€í•˜", "ì˜¤", "ë§", "ì¼", "ê·¸ë ‡", "ìœ„í•˜",
            "ìœ„", "ì•„ë˜", "ì•", "ë’¤", "ì•ˆ", "ë°–", "ì†", "ë°–", "ì‚¬ì´", "ì¤‘ê°„", "ê°€ìš´ë°", "ì–‘ìª½",
            "ëª¨ë“ ", "ì „ì²´", "ì¼ë¶€", "ë¶€ë¶„", "ê°", "ê°œë³„", "ê°œì¸", "ë‹¨ì²´", "ì§‘ë‹¨", "ì¡°ì§", "ê¸°ê´€",
            "íšŒì‚¬", "ê¸°ì—…", "ë‹¨ì²´", "í˜‘íšŒ", "ì¡°í•©", "ì—°í•©", "ì—°ë§¹", "ë™ë§¹", "ì œíœ´", "í˜‘ë ¥", "ê³µì¡°",
            "ìƒí˜¸", "ì–‘ì", "ë‹¤ì", "ì¼ë°©", "ìŒë°©", "ìƒëŒ€", "ë°˜ëŒ€", "ëŒ€ë¦½", "ëŒ€ì¡°", "ë¹„êµ", "ëŒ€ë¹„",
            "ìœ ì‚¬", "ë™ì¼", "ê°™ì€", "ë‹¤ë¥¸", "ì°¨ì´", "êµ¬ë³„", "ë¶„ë³„", "ì‹ë³„", "íŒë³„", "êµ¬ë¶„", "ë¶„ë¥˜",
            "ì •ë ¬", "ë°°ì—´", "ë°°ì¹˜", "ë°°ë¶„", "ë¶„ë°°", "í• ë‹¹", "ì§€ì •", "ì„ ì •", "ì„ íƒ", "ê²°ì •", "íŒë‹¨",
            "ì˜ê²¬", "ìƒê°", "ê´€ì ", "ì…ì¥", "íƒœë„", "ìì„¸", "ë§ˆìŒ", "ì‹¬ì •", "ê°ì •", "ëŠë‚Œ", "ì¸ìƒ",
            "í‰ê°€", "íŒê°€", "ê°€ì¹˜", "ì˜ë¯¸", "ì¤‘ìš”ì„±", "í•„ìš”ì„±", "ë‹¹ìœ„ì„±", "ì •ë‹¹ì„±", "í•©ë¦¬ì„±", "ì ì ˆì„±"
        }
    
    def clean_text(self, text: str) -> str:
        """
        í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        
        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸
            
        Returns:
            ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸
        """
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê³µë°±ë§Œ ìœ ì§€)
        cleaned = re.sub(r'[^\w\sê°€-í£]', ' ', text)
        # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        cleaned = re.sub(r'\s+', ' ', cleaned)
        return cleaned.strip()
    
    def contains_stopword(self, keyword: str, stopwords: set) -> bool:
        """
        í‚¤ì›Œë“œì— ë¶ˆìš©ì–´ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        
        Args:
            keyword: í‚¤ì›Œë“œ
            stopwords: ë¶ˆìš©ì–´ ì§‘í•©
            
        Returns:
            ë¶ˆìš©ì–´ í¬í•¨ ì—¬ë¶€
        """
        # í‚¤ì›Œë“œë¥¼ ë‹¨ì–´ë¡œ ë¶„ë¦¬
        words = keyword.split()
        
        # ê° ë‹¨ì–´ê°€ ë¶ˆìš©ì–´ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        for word in words:
            if word in stopwords:
                return True
        
        # í‚¤ì›Œë“œ ì „ì²´ê°€ ë¶ˆìš©ì–´ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        if keyword in stopwords:
            return True
        
        return False
    
    def extract_keywords_without_stopwords(self, text: str, top_k: int = 5) -> List[str]:
        """
        í•œê¸€ ë¶ˆìš©ì–´ë¥¼ ì œê±°í•˜ê³  KeyBERTë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ
        
        Args:
            text: í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•  í…ìŠ¤íŠ¸
            top_k: ì¶”ì¶œí•  í‚¤ì›Œë“œ ê°œìˆ˜ (ê¸°ë³¸ê°’: 5)
            
        Returns:
            ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        """
        try:
            if self.keybert is None:
                # KeyBERTê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ì¶œ ì‚¬ìš©
                return self.extract_fallback_keywords(text, top_k)
            
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            cleaned_text = self.clean_text(text)
            
            if len(cleaned_text) < 10:  # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ëŠ” ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ì¶œ
                return self.extract_fallback_keywords(text, top_k)
            
            # í•œê¸€ ë¶ˆìš©ì–´ ê°€ì ¸ì˜¤ê¸°
            stopwords = self.get_korean_stopwords()
            
            # KeyBERTë¡œ í‚¤ì›Œë“œ í›„ë³´ ì¶”ì¶œ (ë” ë§ì€ í›„ë³´ ìƒì„±)
            keyword_candidates = self.keybert.extract_keywords(
                cleaned_text,
                keyphrase_ngram_range=(1, 2),  # 1-2ë‹¨ì–´ ì¡°í•©
                stop_words=None,
                top_k=15,  # ìµœëŒ€ 15ê°œ í›„ë³´ ìƒì„±
                diversity=0.8  # ë‹¤ì–‘ì„± ë³´ì¥
            )
            
            if not keyword_candidates:
                return self.extract_fallback_keywords(text, top_k)
            
            # ë¶ˆìš©ì–´ê°€ í¬í•¨ë˜ì§€ ì•Šì€ í‚¤ì›Œë“œë§Œ í•„í„°ë§
            filtered_keywords = []
            for keyword, score in keyword_candidates:
                if not self.contains_stopword(keyword, stopwords):
                    filtered_keywords.append(keyword)
            
            # ìƒìœ„ top_kê°œ í‚¤ì›Œë“œ ë°˜í™˜
            return filtered_keywords[:top_k]
            
        except Exception as e:
            self.logger.error(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return self.extract_fallback_keywords(text, top_k)
    
    def extract_fallback_keywords(self, text: str, top_k: int = 5) -> List[str]:
        """
        KeyBERT ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•  ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ì¶œ
        
        Args:
            text: í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•  í…ìŠ¤íŠ¸
            top_k: ì¶”ì¶œí•  í‚¤ì›Œë“œ ê°œìˆ˜
            
        Returns:
            ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        """
        # ì˜ë£Œ ê´€ë ¨ í‚¤ì›Œë“œ íŒ¨í„´
        medical_keywords = [
            'PSUR', 'ì•ˆì „ì„±', 'ì •ë³´', 'ë³´ê³ ', 'í—ˆê°€', 'í‰ê°€', 'ì˜ë£Œê¸°ê¸°', 'ì˜ì•½í’ˆ',
            'í’ˆì§ˆ', 'ê´€ë¦¬', 'ì‹œì •', 'ì˜ˆë°©', 'ì¡°ì¹˜', 'ì‹¬ì‚¬', 'ì ˆì°¨', 'ì œì¡°ì—…ì²´',
            'ì‹ì•½ì²˜', 'ê°€ì´ë“œë¼ì¸', 'ì„œë¥˜', 'ì²¨ë‹¨ë°”ì´ì˜¤ì˜ì•½í’ˆ', 'ì‹œíŒ', 'í’ˆì§ˆí‰ê°€',
            'ìƒë¬¼í•™ì ', 'íŠ¹ì„±', 'ì œì¡°', 'ê³µì •', 'ë³µì¡ì„±', 'GMP', 'ì‹œìŠ¤í…œ',
            'ê°œì„ ', 'ìœ„í—˜', 'ë°©ì§€', 'ê¸°ìˆ ë¬¸ì„œ', 'ì„ìƒë°ì´í„°', 'ìœ íš¨ì„±', 'ì‹¬ì‚¬ê¸°ê´€',
            'í”„ë¡œíŒŒì¼', 'ëª¨ë‹ˆí„°ë§', 'ë„êµ¬', 'ì¸ì¦', 'ì‹ ê³ ', 'ê³ ë ¤ì‚¬í•­', 'ë°©ë²•',
            'ì ‘ê·¼', 'ê³¼ì •', 'ì…ì¦', 'ìš”êµ¬ì‚¬í•­', 'ìë£Œ', 'ë³´ì™„', 'ê²€ì¦', 'ìŠ¹ì¸',
            'ë“±ë¡', 'ë³€ê²½', 'íì§€', 'ì·¨ì†Œ', 'ì •ì§€', 'ì œí•œ', 'ì¡°ê±´', 'ê¸°ê°„',
            'ë²”ìœ„', 'ëŒ€ìƒ', 'ê¸°ì¤€', 'ê·œì •', 'ì§€ì¹¨', 'ë§¤ë‰´ì–¼', 'ì ˆì°¨ì„œ', 'í‘œì¤€',
            'ê·œê²©', 'ì‚¬ì–‘', 'ì„±ëŠ¥', 'ê¸°ëŠ¥', 'íš¨ê³¼', 'ê²°ê³¼', 'ë¶„ì„', 'ê²€í† ',
            'ê²€ì‚¬', 'ì‹œí—˜', 'ì¸¡ì •', 'ê³„ì‚°', 'ì‚°ì¶œ', 'ë„ì¶œ', 'ê²°ì •', 'íŒë‹¨',
            'ì˜ê²¬', 'ì œì•ˆ', 'ê¶Œê³ ', 'ì§€ì‹œ', 'ëª…ë ¹', 'ìš”ì²­', 'ì‹ ì²­', 'ì œì¶œ',
            'ì ‘ìˆ˜', 'ì²˜ë¦¬', 'ê²€í† ', 'ì‹¬ì˜', 'ì˜ê²°', 'ê²°ì •', 'í†µë³´', 'ê³ ì§€',
            'ê³µê³ ', 'ë°œí‘œ', 'ê³µê°œ', 'ì œê³µ', 'êµë¶€', 'ìˆ˜ë ¹', 'ì ‘ìˆ˜', 'ì²˜ë¦¬',
            # ì••ë ¥ë¶„ì‚° ë§¤íŠ¸ë¦¬ìŠ¤ ê´€ë ¨ í‚¤ì›Œë“œ
            'ì••ë ¥ë¶„ì‚°', 'ë§¤íŠ¸ë¦¬ìŠ¤', 'ìš•ì°½', 'ì˜ˆë°©', 'í”¼ë¶€', 'ì†ìƒ', 'í”¼í•˜ì¡°ì§',
            'ì••ë°•', 'ë¶„ì‚°', 'ì²´ì¤‘', 'ì§€ì§€', 'í‘œë©´', 'ì¬ì§ˆ', 'êµ¬ì¡°', 'ì„¤ê³„',
            'ì•ˆì •ì„±', 'ë‚´êµ¬ì„±', 'í¸ì•ˆí•¨', 'ì§€ì§€ë ¥', 'ë¶„ì‚°ë ¥', 'ì••ë ¥', 'ë¶„í¬'
        ]
        
        # í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì°¾ê¸°
        found_keywords = []
        for keyword in medical_keywords:
            if keyword in text:
                found_keywords.append(keyword)
        
        # ìƒìœ„ top_kê°œë§Œ ë°˜í™˜
        return found_keywords[:top_k]
    
    def process_json_file(self, input_file: str, output_file: str = None) -> str:
        """
        JSON íŒŒì¼ì˜ ëª¨ë“  ì²­í¬ì— í‚¤ì›Œë“œ ì¶”ê°€
        
        Args:
            input_file: ì…ë ¥ JSON íŒŒì¼ ê²½ë¡œ
            output_file: ì¶œë ¥ JSON íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ìë™ ìƒì„±)
            
        Returns:
            ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
        """
        input_path = Path(input_file)
        
        if not input_path.exists():
            raise FileNotFoundError(f"ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_file}")
        
        # ì¶œë ¥ íŒŒì¼ëª… ì„¤ì •
        if output_file is None:
            output_path = input_path.parent / "with_keywords.json"
        else:
            output_path = Path(output_file)
        
        self.logger.info(f"ğŸ“– JSON íŒŒì¼ ë¡œë”©: {input_file}")
        
        try:
            # JSON íŒŒì¼ ë¡œë“œ
            with open(input_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if not isinstance(data, list):
                raise ValueError("JSON íŒŒì¼ì€ ë°°ì—´ í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤.")
            
            self.logger.info(f"ğŸ“Š ì´ {len(data)}ê°œ ì²­í¬ ì²˜ë¦¬ ì‹œì‘")
            
            # ê° ì²­í¬ì— í‚¤ì›Œë“œ ì¶”ê°€
            processed_data = []
            for i, chunk in enumerate(data):
                if i % 10 == 0:  # ì§„í–‰ìƒí™© ë¡œê·¸
                    self.logger.info(f"ì²˜ë¦¬ ì¤‘... {i+1}/{len(data)}")
                
                # ì²­í¬ ë³µì‚¬
                processed_chunk = chunk.copy()
                
                # text í•„ë“œ í™•ì¸
                if 'text' not in processed_chunk:
                    self.logger.warning(f"ì²­í¬ {i}ì— 'text' í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
                    processed_chunk['keywords'] = []
                    processed_data.append(processed_chunk)
                    continue
                
                # í‚¤ì›Œë“œ ì¶”ì¶œ (í•œê¸€ ë¶ˆìš©ì–´ ì œê±°)
                text = processed_chunk['text']
                keywords = self.extract_keywords_without_stopwords(text, top_k=5)
                
                # í‚¤ì›Œë“œ í•„ë“œ ì¶”ê°€
                processed_chunk['keywords'] = keywords
                processed_data.append(processed_chunk)
            
            # ê²°ê³¼ ì €ì¥
            self.logger.info(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(processed_data, f, ensure_ascii=False, indent=2)
            
            self.logger.info("âœ… í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ!")
            return str(output_path)
            
        except Exception as e:
            self.logger.error(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='í•œê¸€ ë¶ˆìš©ì–´ ì œê±° KeyBERT í‚¤ì›Œë“œ ì¶”ì¶œ')
    parser.add_argument('--input', '-i', required=True, help='ì…ë ¥ JSON íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--output', '-o', help='ì¶œë ¥ JSON íŒŒì¼ ê²½ë¡œ (ì„ íƒì‚¬í•­)')
    parser.add_argument('--model', '-m', default='all-MiniLM-L6-v2', help='ì‚¬ìš©í•  ëª¨ë¸ëª…')
    parser.add_argument('--top-k', '-k', type=int, default=5, help='ì¶”ì¶œí•  í‚¤ì›Œë“œ ê°œìˆ˜')
    
    args = parser.parse_args()
    
    try:
        # í‚¤ì›Œë“œ ì¶”ì¶œê¸° ì´ˆê¸°í™”
        extractor = KoreanKeywordExtractor(model_name=args.model)
        
        # íŒŒì¼ ì²˜ë¦¬
        output_file = extractor.process_json_file(args.input, args.output)
        
        print(f"\nğŸ‰ í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ!")
        print(f"ğŸ“ ì¶œë ¥ íŒŒì¼: {output_file}")
        
    except Exception as e:
        logger.error(f"ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main()) 