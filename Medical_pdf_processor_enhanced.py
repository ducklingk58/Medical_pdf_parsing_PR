"""
í–¥ìƒëœ Medical PDF íŒŒì‹± í”„ë¡œì„¸ì„œ
Unstructured í…ìŠ¤íŠ¸ íŒŒì‹± + LayoutParser/PaddleOCR ê³ ê¸‰ í…Œì´ë¸”/ì´ë¯¸ì§€ íŒŒì‹± í†µí•©
"""

import os
import re
import json
import pandas as pd
import io
import time
import traceback
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
from sentence_transformers import SentenceTransformer
import numpy as np
from unstructured.partition.auto import partition
from unstructured.documents.elements import Table, Title, NarrativeText, Image
import logging
from tqdm import tqdm
import concurrent.futures
from datetime import datetime

# ê³ ê¸‰ íŒŒì‹± ëª¨ë“ˆ import
from utils.advanced_parser import AdvancedParser
from config.settings import Settings
from utils.logger import setup_logger, log_file_processing

class EnhancedPDFProcessor:
    """í–¥ìƒëœ PDF ì²˜ë¦¬ê¸° - Unstructured + LayoutParser/PaddleOCR í†µí•©"""
    
    def __init__(self, use_gpu: bool = False, similarity_threshold: float = None):
        self.similarity_threshold = similarity_threshold or Settings.SIMILARITY_THRESHOLD
        self.sentence_model = SentenceTransformer(Settings.SENTENCE_MODEL)
        self.logger = logging.getLogger(__name__)
        self.use_gpu = use_gpu
        
        # ê³ ê¸‰ íŒŒì‹±ê¸° ì´ˆê¸°í™”
        self.advanced_parser = AdvancedParser(use_gpu=use_gpu)
        
        self.logger.info(f"í–¥ìƒëœ PDF ì²˜ë¦¬ê¸° ì´ˆê¸°í™” ì™„ë£Œ (GPU: {use_gpu})")
    
    def partition_pdf_text_only(self, pdf_path: str) -> List:
        """Unstructuredë¥¼ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ì „ìš© íŒŒì‹± (í…Œì´ë¸”/ì´ë¯¸ì§€ ì¶”ì¶œ ë¹„í™œì„±í™”)"""
        try:
            self.logger.info(f"í…ìŠ¤íŠ¸ ì „ìš© PDF íŒŒì‹± ì‹œì‘: {pdf_path}")
            elements = partition(
                filename=pdf_path,
                include_image_metadata=False,  # ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ë¹„í™œì„±í™”
                extract_tables=False,          # í…Œì´ë¸” ì¶”ì¶œ ë¹„í™œì„±í™”
                pdf_extract_images=False,      # ì´ë¯¸ì§€ ì¶”ì¶œ ë¹„í™œì„±í™”
                strategy="fast"
            )
            self.logger.info(f"í…ìŠ¤íŠ¸ íŒŒì‹± ì™„ë£Œ: {len(elements)}ê°œ ìš”ì†Œ ì¶”ì¶œ")
            return elements
        except Exception as e:
            self.logger.error(f"í…ìŠ¤íŠ¸ íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
            raise
    
    def clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ì œ - í—¤ë”/í‘¸í„°, í˜ì´ì§€ ë²ˆí˜¸ ì œê±°"""
        if not text:
            return ""
        
        # í˜ì´ì§€ ë²ˆí˜¸ íŒ¨í„´ ì œê±°
        if Settings.REMOVE_PAGE_NUMBERS:
            text = re.sub(r'---\s*PAGE\s+\d+\s*---', '', text, flags=re.IGNORECASE)
            text = re.sub(r'-\s*\d+\s*-', '', text)
            text = re.sub(r'í˜ì´ì§€\s*\d+', '', text)
            text = re.sub(r'Page\s+\d+', '', text, flags=re.IGNORECASE)
        
        # ë°˜ë³µë˜ëŠ” í—¤ë”/í‘¸í„° ì œê±°
        if Settings.REMOVE_HEADERS_FOOTERS:
            text = re.sub(r'MFDS/MaPP.*?\n', '', text, flags=re.IGNORECASE)
            text = re.sub(r'ì˜ì•½í’ˆì•ˆì „ê´€ë¦¬.*?\n', '', text)
            text = re.sub(r'ì‹í’ˆì˜ì•½í’ˆì•ˆì „ì²˜.*?\n', '', text)
        
        # ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬
        text = re.sub(r'\n\s*\n', '\n\n', text)
        text = re.sub(r' +', ' ', text)
        text = re.sub(r'\t+', ' ', text)
        
        return text.strip()
    
    def extract_text_elements(self, elements: List) -> List[str]:
        """í…ìŠ¤íŠ¸ ìš”ì†Œ ì¶”ì¶œ ë° ì •ì œ"""
        text_elements = []
        
        for element in elements:
            if isinstance(element, (NarrativeText, Title)):
                cleaned_text = self.clean_text(element.text)
                if cleaned_text:
                    text_elements.append(cleaned_text)
        
        self.logger.info(f"ì´ {len(text_elements)}ê°œ í…ìŠ¤íŠ¸ ìš”ì†Œ ì¶”ì¶œ ì™„ë£Œ")
        return text_elements
    
    def calculate_similarity(self, text1: str, text2: str) -> float:
        """ë‘ í…ìŠ¤íŠ¸ ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        try:
            embeddings = self.sentence_model.encode([text1, text2])
            similarity = np.dot(embeddings[0], embeddings[1]) / (
                np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1])
            )
            return float(similarity)
        except Exception as e:
            self.logger.warning(f"ìœ ì‚¬ë„ ê³„ì‚° ì˜¤ë¥˜: {str(e)}")
            return 0.0
    
    def connect_sentences(self, text_elements: List[str]) -> List[str]:
        """ì˜ë¯¸ë¡ ì  ìœ ì‚¬ë„ ê¸°ë°˜ ë¬¸ì¥ ì—°ê²°"""
        if not text_elements:
            return []
        
        self.logger.info("ë¬¸ì¥ ì—°ê²° í”„ë¡œì„¸ìŠ¤ ì‹œì‘...")
        connected_blocks = []
        current_block = text_elements[0]
        
        for i in range(1, len(text_elements)):
            current_text = text_elements[i]
            
            # ë…¼ë¦¬ì  ë‹¨ì ˆ ì§€ì  ì‹ë³„
            is_break_point = self._is_logical_break(current_block, current_text)
            
            if is_break_point:
                # ë‹¨ì ˆ ì§€ì ì´ë©´ ìƒˆ ë¸”ë¡ ì‹œì‘
                if current_block.strip():
                    connected_blocks.append(current_block.strip())
                current_block = current_text
            else:
                # ìœ ì‚¬ë„ ê³„ì‚°
                similarity = self.calculate_similarity(current_block, current_text)
                
                if similarity >= self.similarity_threshold:
                    # ìœ ì‚¬ë„ê°€ ë†’ìœ¼ë©´ ì—°ê²°
                    current_block += "\n\n" + current_text
                else:
                    # ìœ ì‚¬ë„ê°€ ë‚®ìœ¼ë©´ ìƒˆ ë¸”ë¡ ì‹œì‘
                    if current_block.strip():
                        connected_blocks.append(current_block.strip())
                    current_block = current_text
        
        # ë§ˆì§€ë§‰ ë¸”ë¡ ì¶”ê°€
        if current_block.strip():
            connected_blocks.append(current_block.strip())
        
        self.logger.info(f"ë¬¸ì¥ ì—°ê²° ì™„ë£Œ: {len(connected_blocks)}ê°œ ë¸”ë¡ ìƒì„±")
        return connected_blocks
    
    def _is_logical_break(self, text1: str, text2: str) -> bool:
        """ë…¼ë¦¬ì  ë‹¨ì ˆ ì§€ì  ì‹ë³„"""
        # ì œëª© íŒ¨í„´ í™•ì¸
        if re.match(r'^[A-Z][A-Z\s]+$', text2.strip()):
            return True
        
        # ë²ˆí˜¸ íŒ¨í„´ í™•ì¸
        if re.match(r'^\d+\.\s', text2.strip()):
            return True
        
        # ì†Œì œëª© íŒ¨í„´ í™•ì¸
        if re.match(r'^[ê°€-í£\s]+:$', text2.strip()):
            return True
        
        # ì§§ì€ í…ìŠ¤íŠ¸ëŠ” ë‹¨ì ˆ ê°€ëŠ¥ì„± ë†’ìŒ
        if len(text2.strip()) < 20:
            return True
        
        return False
    
    def create_rag_chunks(self, text_blocks: List[str], chunk_size: int = None, overlap: int = None) -> List[Dict[str, Any]]:
        """RAG ì¹œí™”ì  ì²­í‚¹ ìƒì„±"""
        chunk_size = chunk_size or Settings.CHUNK_SIZE
        overlap = overlap or Settings.CHUNK_OVERLAP
        
        chunks = []
        
        for block in text_blocks:
            # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
            sentences = re.split(r'[.!?]+', block)
            
            current_chunk = ""
            for sentence in sentences:
                if len(current_chunk) + len(sentence) <= chunk_size:
                    current_chunk += sentence + " "
                else:
                    if current_chunk.strip():
                        chunks.append({
                            "text": current_chunk.strip(),
                            "token_estimate": len(current_chunk.split()),
                            "type": "text_chunk"
                        })
                    current_chunk = sentence + " "
            
            if current_chunk.strip():
                chunks.append({
                    "text": current_chunk.strip(),
                    "token_estimate": len(current_chunk.split()),
                    "type": "text_chunk"
                })
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
        final_chunks = []
        for i, chunk in enumerate(chunks):
            if i > 0 and overlap > 0:
                # ì´ì „ ì²­í¬ì™€ ì¤‘ë³µ ì¶”ê°€
                prev_text = chunks[i-1]["text"]
                overlap_text = " ".join(prev_text.split()[-overlap:])
                chunk["text"] = overlap_text + " " + chunk["text"]
            
            final_chunks.append(chunk)
        
        self.logger.info(f"RAG ì²­í‚¹ ì™„ë£Œ: {len(final_chunks)}ê°œ ì²­í¬ ìƒì„±")
        return final_chunks
    
    def save_advanced_tables(self, extracted_tables: List[Dict], output_dir: Path) -> List[str]:
        """ê³ ê¸‰ íŒŒì‹±ìœ¼ë¡œ ì¶”ì¶œëœ í…Œì´ë¸” ì €ì¥"""
        table_files = []
        tables_dir = output_dir / "tables"
        tables_dir.mkdir(exist_ok=True)
        
        for table in extracted_tables:
            table_id = table.get("table_id", f"table_{len(table_files)+1:03d}")
            
            # JSON í˜•íƒœë¡œ ì €ì¥
            json_path = tables_dir / f"{table_id}.json"
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(table, f, ensure_ascii=False, indent=2, default=str)
            
            # CSV í˜•íƒœë¡œë„ ì €ì¥ (DataFrameì´ ìˆëŠ” ê²½ìš°)
            if "dataframe" in table and table["dataframe"] is not None:
                csv_path = tables_dir / f"{table_id}.csv"
                table["dataframe"].to_csv(csv_path, index=False, encoding='utf-8-sig')
                
                # Excel í˜•íƒœë¡œë„ ì €ì¥
                excel_path = tables_dir / f"{table_id}.xlsx"
                table["dataframe"].to_excel(excel_path, index=False, engine='openpyxl')
            
            # HTML í˜•íƒœë¡œë„ ì €ì¥
            if "html" in table:
                html_path = tables_dir / f"{table_id}.html"
                with open(html_path, 'w', encoding='utf-8') as f:
                    f.write(table["html"])
            
            table_files.append(str(json_path))
        
        self.logger.info(f"ê³ ê¸‰ í…Œì´ë¸” ë°ì´í„° ì €ì¥ ì™„ë£Œ: {len(table_files)}ê°œ íŒŒì¼")
        return table_files
    
    def save_advanced_images(self, extracted_images: List[Dict], output_dir: Path) -> List[str]:
        """ê³ ê¸‰ íŒŒì‹±ìœ¼ë¡œ ì¶”ì¶œëœ ì´ë¯¸ì§€ ì €ì¥"""
        image_files = []
        images_dir = output_dir / "images"
        images_dir.mkdir(exist_ok=True)
        
        for image in extracted_images:
            image_id = image.get("image_id", f"image_{len(image_files)+1:03d}")
            
            # ì´ë¯¸ì§€ íŒŒì¼ ì €ì¥
            if "image_data" in image:
                image_path = images_dir / f"{image_id}.png"
                with open(image_path, 'wb') as f:
                    f.write(image["image_data"])
            
            # ë©”íƒ€ë°ì´í„° JSON ì €ì¥
            metadata_path = images_dir / f"{image_id}_metadata.json"
            metadata = {k: v for k, v in image.items() if k != "image_data"}
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2, default=str)
            
            image_files.append(str(metadata_path))
        
        self.logger.info(f"ê³ ê¸‰ ì´ë¯¸ì§€ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {len(image_files)}ê°œ íŒŒì¼")
        return image_files
    
    def create_enhanced_markdown(self, text_blocks: List[str], table_files: List[str], 
                                image_files: List[str], output_dir: Path) -> str:
        """í–¥ìƒëœ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ìƒì„±"""
        md_content = []
        
        # í…ìŠ¤íŠ¸ ë¸”ë¡ ì¶”ê°€
        for i, block in enumerate(text_blocks):
            md_content.append(f"## í…ìŠ¤íŠ¸ ë¸”ë¡ {i+1}\n")
            md_content.append(block)
            md_content.append("\n\n")
        
        # ê³ ê¸‰ í…Œì´ë¸” ì°¸ì¡° ì¶”ê°€
        if table_files:
            md_content.append("## ì¶”ì¶œëœ í…Œì´ë¸” (ê³ ê¸‰ íŒŒì‹±)\n")
            for table_file in table_files:
                table_name = Path(table_file).stem
                table_dir = Path(table_file).parent
                
                # CSV íŒŒì¼ì´ ìˆìœ¼ë©´ ë§í¬ ì¶”ê°€
                csv_file = table_dir / f"{table_name}.csv"
                if csv_file.exists():
                    md_content.append(f"- [í…Œì´ë¸” ë°ì´í„° (CSV): {table_name}]({csv_file.relative_to(output_dir)})\n")
                
                # Excel íŒŒì¼ì´ ìˆìœ¼ë©´ ë§í¬ ì¶”ê°€
                excel_file = table_dir / f"{table_name}.xlsx"
                if excel_file.exists():
                    md_content.append(f"- [í…Œì´ë¸” ë°ì´í„° (Excel): {table_name}]({excel_file.relative_to(output_dir)})\n")
                
                # HTML íŒŒì¼ì´ ìˆìœ¼ë©´ ë§í¬ ì¶”ê°€
                html_file = table_dir / f"{table_name}.html"
                if html_file.exists():
                    md_content.append(f"- [í…Œì´ë¸” ë°ì´í„° (HTML): {table_name}]({html_file.relative_to(output_dir)})\n")
                
                # JSON ë©”íƒ€ë°ì´í„° ë§í¬
                md_content.append(f"- [í…Œì´ë¸” ë©”íƒ€ë°ì´í„°: {table_name}]({table_file})\n")
            md_content.append("\n")
        
        # ê³ ê¸‰ ì´ë¯¸ì§€ ì°¸ì¡° ì¶”ê°€
        if image_files:
            md_content.append("## ì¶”ì¶œëœ ì´ë¯¸ì§€ (ê³ ê¸‰ íŒŒì‹±)\n")
            for image_file in image_files:
                image_name = Path(image_file).stem.replace("_metadata", "")
                image_dir = Path(image_file).parent
                
                # ì‹¤ì œ ì´ë¯¸ì§€ íŒŒì¼ ë§í¬
                image_path = image_dir / f"{image_name}.png"
                if image_path.exists():
                    md_content.append(f"![{image_name}]({image_path.relative_to(output_dir)})\n")
                
                # ë©”íƒ€ë°ì´í„° ë§í¬
                md_content.append(f"- [ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„°: {image_name}]({image_file})\n")
            md_content.append("\n")
        
        md_path = output_dir / "enhanced_markdown.md"
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(''.join(md_content))
        
        self.logger.info(f"í–¥ìƒëœ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ìƒì„± ì™„ë£Œ: {md_path}")
        return str(md_path)
    
    def create_enhanced_summary(self, text_blocks: List[str], extracted_tables: List[Dict], 
                               extracted_images: List[Dict], chunks: List[Dict]) -> Dict[str, Any]:
        """í–¥ìƒëœ íŒŒì‹± ê²°ê³¼ ìš”ì•½ ìƒì„±"""
        total_words = sum(len(block.split()) for block in text_blocks)
        total_tokens = sum(chunk.get("token_estimate", 0) for chunk in chunks)
        
        # í…Œì´ë¸” í†µê³„
        table_stats = {
            "total_tables": len(extracted_tables),
            "tables_with_dataframe": len([t for t in extracted_tables if t.get("dataframe") is not None]),
            "tables_with_html": len([t for t in extracted_tables if t.get("html")]),
            "total_table_rows": sum(t.get("shape", [0, 0])[0] for t in extracted_tables if t.get("shape")),
            "total_table_columns": sum(t.get("shape", [0, 0])[1] for t in extracted_tables if t.get("shape"))
        }
        
        # ì´ë¯¸ì§€ í†µê³„
        image_stats = {
            "total_images": len(extracted_images),
            "images_with_data": len([i for i in extracted_images if i.get("image_data")]),
            "total_image_size_mb": sum(i.get("size", 0) for i in extracted_images) / (1024 * 1024)
        }
        
        summary = {
            "success": True,  # ì„±ê³µ ìƒíƒœ í•„ë“œ ì¶”ê°€
            "text_blocks_count": len(text_blocks),
            "rag_chunks_count": len(chunks),
            "total_words": total_words,
            "total_tokens_estimate": total_tokens,
            "table_statistics": table_stats,
            "image_statistics": image_stats,
            "processing_timestamp": pd.Timestamp.now().isoformat(),
            "processing_method": "Enhanced (Unstructured + Table Transformer)",
            "settings": {
                "similarity_threshold": self.similarity_threshold,
                "chunk_size": Settings.CHUNK_SIZE,
                "chunk_overlap": Settings.CHUNK_OVERLAP,
                "use_gpu": self.use_gpu
            }
        }
        
        return summary
    
    def process_pdf_enhanced(self, pdf_path: str, output_dir: str) -> Dict[str, Any]:
        """í–¥ìƒëœ PDF ì²˜ë¦¬ ë©”ì¸ í”„ë¡œì„¸ìŠ¤"""
        try:
            self.logger.info(f"í–¥ìƒëœ PDF ì²˜ë¦¬ ì‹œì‘: {pdf_path}")
            output_path = Path(output_dir)
            output_path.mkdir(parents=True, exist_ok=True)
            
            start_time = time.time()
            
            # 1ë‹¨ê³„: ì¼ë°˜ í…ìŠ¤íŠ¸ íŒŒì‹± (Unstructured)
            self.logger.info("1ë‹¨ê³„: ì¼ë°˜ í…ìŠ¤íŠ¸ íŒŒì‹± (Unstructured)")
            print("ğŸ”¤ ì¼ë°˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
            elements = self.partition_pdf_text_only(pdf_path)
            text_elements = self.extract_text_elements(elements)
            print(f"   âœ… {len(text_elements)}ê°œ í…ìŠ¤íŠ¸ ìš”ì†Œ ì¶”ì¶œ ì™„ë£Œ")
            
            # 2ë‹¨ê³„: í‘œ ë° ì´ë¯¸ì§€ íŒŒì‹± (Table Transformer + LayoutParser)
            self.logger.info("2ë‹¨ê³„: í‘œ ë° ì´ë¯¸ì§€ íŒŒì‹± (Table Transformer + LayoutParser)")
            print("ğŸ“Š í‘œ ë°ì´í„° íŒŒì‹± ì¤‘ (Table Transformer)...")
            advanced_result = self.advanced_parser.process_pdf_advanced(pdf_path, str(output_path))
            print(f"   âœ… {len(advanced_result.get('tables', []))}ê°œ í‘œ, {len(advanced_result.get('images', []))}ê°œ ì´ë¯¸ì§€ ì¶”ì¶œ ì™„ë£Œ")
            
            # 3ë‹¨ê³„: í…ìŠ¤íŠ¸ ì •ì œ ë° ë¬¸ì¥ ì—°ê²°
            self.logger.info("3ë‹¨ê³„: í…ìŠ¤íŠ¸ ì •ì œ ë° ë¬¸ì¥ ì—°ê²°")
            print("ğŸ”— í…ìŠ¤íŠ¸ ë¸”ë¡ ì—°ê²° ì¤‘...")
            connected_blocks = self.connect_sentences(text_elements)
            print(f"   âœ… {len(connected_blocks)}ê°œ ì—°ê²°ëœ í…ìŠ¤íŠ¸ ë¸”ë¡ ìƒì„±")
            
            # 4ë‹¨ê³„: RAG ì²­í‚¹
            self.logger.info("4ë‹¨ê³„: RAG ì²­í‚¹")
            print("âœ‚ï¸ RAG ìµœì í™” ì²­í‚¹ ì¤‘...")
            rag_chunks = self.create_rag_chunks(connected_blocks)
            print(f"   âœ… {len(rag_chunks)}ê°œ RAG ì²­í¬ ìƒì„±")
            
            # 5ë‹¨ê³„: íŒŒì¼ ì €ì¥ ë° êµ¬ì¡°í™”
            self.logger.info("5ë‹¨ê³„: íŒŒì¼ ì €ì¥ ë° êµ¬ì¡°í™”")
            print("ğŸ’¾ ê²°ê³¼ íŒŒì¼ ì €ì¥ ì¤‘...")
            table_files = self.save_advanced_tables(advanced_result["tables"], output_path)
            image_files = self.save_advanced_images(advanced_result["images"], output_path)
            
            # 6ë‹¨ê³„: ë§ˆí¬ë‹¤ìš´ ìƒì„±
            self.logger.info("6ë‹¨ê³„: ë§ˆí¬ë‹¤ìš´ ìƒì„±")
            md_file = self.create_enhanced_markdown(connected_blocks, table_files, image_files, output_path)
            
            # 7ë‹¨ê³„: êµ¬ì¡°í™”ëœ JSON ìƒì„± (ì‚¬ìš©ì ìŠ¤í‚¤ë§ˆ ì ìš©)
            self.logger.info("7ë‹¨ê³„: êµ¬ì¡°í™”ëœ JSON ìƒì„± (ì‚¬ìš©ì ìŠ¤í‚¤ë§ˆ ì ìš©)")
            print("ğŸ“„ êµ¬ì¡°í™”ëœ JSON ê²°ê³¼ ìƒì„± ì¤‘...")
            
            # ê¸°ì¡´ êµ¬ì¡°í™”ëœ JSON ìƒì„±ê¸° ì‚¬ìš©
            try:
                from utils.structured_json_generator import create_structured_json
                
                # í…ìŠ¤íŠ¸ ì²­í¬ì™€ í‘œ ë°ì´í„°ë¡œë¶€í„° êµ¬ì¡°í™”ëœ ë¬¸ì„œ ìƒì„±
                structured_document = create_structured_json(
                    text_chunks=rag_chunks,  # RAG ì²­í¬ ì‚¬ìš©
                    extracted_tables=advanced_result["tables"],
                    source_file=pdf_path
                )
                
                # êµ¬ì¡°í™”ëœ ë¬¸ì„œ ì €ì¥
                structured_path = output_path / "structured_document.json"
                with open(structured_path, 'w', encoding='utf-8') as f:
                    json.dump(structured_document, f, ensure_ascii=False, indent=2, default=str)
                print(f"   âœ… êµ¬ì¡°í™”ëœ ë¬¸ì„œ ì €ì¥: {structured_path}")
                
            except ImportError:
                self.logger.warning("êµ¬ì¡°í™”ëœ JSON ìƒì„±ê¸°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                print("   âš ï¸ êµ¬ì¡°í™”ëœ JSON ìƒì„± ì‹¤íŒ¨, ê¸°ë³¸ í˜•íƒœë¡œ ì €ì¥")
            
            # ì‚¬ìš©ì ì •ì˜ ìŠ¤í‚¤ë§ˆ JSON ìƒì„± (ìƒˆë¡œ ì¶”ê°€)
            try:
                from utils.user_schema_generator import create_user_schema_json
                
                print("ğŸ“‹ ì‚¬ìš©ì ì •ì˜ ìŠ¤í‚¤ë§ˆ JSON ìƒì„± ì¤‘...")
                
                # ì‚¬ìš©ì ìŠ¤í‚¤ë§ˆì— ë§ëŠ” ë°°ì—´ í˜•íƒœ JSON ìƒì„±
                user_schema_json = create_user_schema_json(
                    text_chunks=rag_chunks,  # RAG ì²­í¬ ì‚¬ìš©
                    extracted_tables=advanced_result["tables"],
                    source_file=pdf_path
                )
                
                # ì‚¬ìš©ì ìŠ¤í‚¤ë§ˆ JSON ì €ì¥
                user_schema_path = output_path / "user_schema_output.json"
                with open(user_schema_path, 'w', encoding='utf-8') as f:
                    json.dump(user_schema_json, f, ensure_ascii=False, indent=2, default=str)
                print(f"   âœ… ì‚¬ìš©ì ì •ì˜ ìŠ¤í‚¤ë§ˆ JSON ì €ì¥: {user_schema_path}")
                
            except ImportError:
                self.logger.warning("ì‚¬ìš©ì ìŠ¤í‚¤ë§ˆ JSON ìƒì„±ê¸°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                print("   âš ï¸ ì‚¬ìš©ì ìŠ¤í‚¤ë§ˆ JSON ìƒì„± ì‹¤íŒ¨")
                
                # ê¸°ë³¸ ë©”íƒ€ë°ì´í„° (text_blocks ì œì™¸)
                metadata = {
                    "source_file": pdf_path,
                    "rag_chunks": rag_chunks,
                    "extracted_tables": advanced_result["tables"],
                    "extracted_images": advanced_result["images"],
                    "processing_info": {
                        "total_elements": len(elements),
                        "text_elements": len(text_elements),
                        "connected_blocks": len(connected_blocks),
                        "processing_time": time.time() - start_time,
                        "method": "Enhanced (Unstructured + Table Transformer)"
                    }
                }
                metadata_path = output_path / "enhanced_metadata.json"
                with open(metadata_path, 'w', encoding='utf-8') as f:
                    json.dump(metadata, f, ensure_ascii=False, indent=2, default=str)
                print(f"   âœ… ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_path}")
            
            # 8ë‹¨ê³„: í‚¤ì›Œë“œ ì¶”ì¶œ ë° ì¶”ê°€
            self.logger.info("8ë‹¨ê³„: í‚¤ì›Œë“œ ì¶”ì¶œ ë° ì¶”ê°€")
            print("ğŸ” í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
            
            try:
                # KeyBERT ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œê¸° ì‚¬ìš©
                from keyword_extractor import KeywordExtractor
                
                # í‚¤ì›Œë“œ ì¶”ì¶œê¸° ì´ˆê¸°í™”
                keyword_extractor = KeywordExtractor()
                
                # ë¬¸ì„œ ì œëª© ì¶”ì¶œ
                def extract_document_title(chunks):
                    """ë¬¸ì„œ ì œëª© ì¶”ì¶œ"""
                    if chunks and len(chunks) > 0:
                        first_chunk = chunks[0]
                        if 'metadata' in first_chunk and 'heading' in first_chunk['metadata']:
                            heading = first_chunk['metadata']['heading']
                            if heading and len(heading.strip()) > 0:
                                return heading.strip()
                    return "ì˜ë£Œê¸°ê¸° ê°€ì´ë“œë¼ì¸"
                
                # ë¬¸ì„œ ì œëª© ì¶”ì¶œ
                document_title = extract_document_title(user_schema_json if 'user_schema_json' in locals() else rag_chunks)
                print(f"ğŸ“‹ ë¬¸ì„œ ì œëª©: {document_title}")
                
                # í‚¤ì›Œë“œ ê°œìˆ˜ ì„¤ì • (ê¸°ë³¸ê°’: 5ê°œ)
                keyword_count = 5
                
                # ì‚¬ìš©ì ìŠ¤í‚¤ë§ˆ JSONì— í‚¤ì›Œë“œ ì¶”ê°€
                if 'user_schema_json' in locals():
                    print("ğŸ“‹ ì‚¬ìš©ì ìŠ¤í‚¤ë§ˆ JSONì— í‚¤ì›Œë“œ ì¶”ê°€ ì¤‘...")
                    
                    # ê° ì²­í¬ì— í‚¤ì›Œë“œ ì¶”ê°€
                    for chunk in user_schema_json:
                        if 'text' in chunk:
                            keywords = keyword_extractor.extract_keywords_with_title_similarity(
                                chunk['text'], document_title, top_k=keyword_count
                            )
                            chunk['keywords'] = keywords
                    
                    # í‚¤ì›Œë“œê°€ ì¶”ê°€ëœ ì‚¬ìš©ì ìŠ¤í‚¤ë§ˆ JSON ì €ì¥
                    user_schema_with_keywords_path = output_path / "user_schema_with_keywords.json"
                    with open(user_schema_with_keywords_path, 'w', encoding='utf-8') as f:
                        json.dump(user_schema_json, f, ensure_ascii=False, indent=2, default=str)
                    print(f"   âœ… í‚¤ì›Œë“œê°€ ì¶”ê°€ëœ ì‚¬ìš©ì ìŠ¤í‚¤ë§ˆ JSON ì €ì¥: {user_schema_with_keywords_path}")
                
                # RAG ì²­í¬ì—ë„ í‚¤ì›Œë“œ ì¶”ê°€
                print("ğŸ“Š RAG ì²­í¬ì— í‚¤ì›Œë“œ ì¶”ê°€ ì¤‘...")
                for chunk in rag_chunks:
                    if 'text' in chunk:
                        keywords = keyword_extractor.extract_keywords_with_title_similarity(
                            chunk['text'], document_title, top_k=keyword_count
                        )
                        chunk['keywords'] = keywords
                
                # í‚¤ì›Œë“œê°€ ì¶”ê°€ëœ RAG ì²­í¬ ì €ì¥
                rag_chunks_with_keywords_path = output_path / "rag_chunks_with_keywords.json"
                with open(rag_chunks_with_keywords_path, 'w', encoding='utf-8') as f:
                    json.dump(rag_chunks, f, ensure_ascii=False, indent=2, default=str)
                print(f"   âœ… í‚¤ì›Œë“œê°€ ì¶”ê°€ëœ RAG ì²­í¬ ì €ì¥: {rag_chunks_with_keywords_path}")
                
                # ë©”ì¸ metadata.json íŒŒì¼ì— í‚¤ì›Œë“œê°€ ì¶”ê°€ëœ ì‚¬ìš©ì ìŠ¤í‚¤ë§ˆ JSON ì €ì¥
                if 'user_schema_json' in locals():
                    print("ğŸ“‹ ë©”ì¸ metadata.json íŒŒì¼ì— í‚¤ì›Œë“œ ì¶”ê°€ ì¤‘...")
                    metadata_path = output_path / "metadata.json"
                    with open(metadata_path, 'w', encoding='utf-8') as f:
                        json.dump(user_schema_json, f, ensure_ascii=False, indent=2, default=str)
                    print(f"   âœ… í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë©”ì¸ metadata.json ì €ì¥: {metadata_path}")
                
            except ImportError:
                self.logger.warning("í‚¤ì›Œë“œ ì¶”ì¶œê¸°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                print("   âš ï¸ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨")
            except Exception as e:
                self.logger.error(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                print(f"   âŒ í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            
            # 9ë‹¨ê³„: ìµœì¢… ìš”ì•½ ìƒì„±
            self.logger.info("9ë‹¨ê³„: ìµœì¢… ìš”ì•½ ìƒì„±")
            print("ğŸ“Š ìµœì¢… ìš”ì•½ ìƒì„± ì¤‘...")
            summary = self.create_enhanced_summary(
                connected_blocks, 
                advanced_result["tables"], 
                advanced_result["images"], 
                rag_chunks
            )
            summary_path = output_path / "enhanced_summary.json"
            with open(summary_path, 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2, default=str)
            print(f"   âœ… ìš”ì•½ ì €ì¥: {summary_path}")
            
            processing_time = time.time() - start_time
            print(f"ğŸ‰ í–¥ìƒëœ PDF ì²˜ë¦¬ ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {processing_time:.2f}ì´ˆ)")
            self.logger.info(f"í–¥ìƒëœ PDF ì²˜ë¦¬ ì™„ë£Œ: {processing_time:.2f}ì´ˆ")
            
            return summary
            
        except Exception as e:
            self.logger.error(f"í–¥ìƒëœ PDF ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            raise
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if hasattr(self, 'advanced_parser'):
            self.advanced_parser.cleanup()
    
    def __del__(self):
        """ì†Œë©¸ìì—ì„œ ì •ë¦¬"""
        self.cleanup()

class EnhancedBatchProcessor:
    """í–¥ìƒëœ ë°°ì¹˜ ì²˜ë¦¬ê¸°"""
    
    def __init__(self, input_dir: str = None, output_dir: str = None, 
                 max_workers: int = None, use_gpu: bool = False):
        self.input_dir = Path(input_dir) if input_dir else Settings.INPUT_DIR
        self.output_dir = Path(output_dir) if output_dir else Settings.OUTPUT_DIR
        self.max_workers = max_workers or Settings.MAX_WORKERS
        self.use_gpu = use_gpu
        self.logger = setup_logger()
        
        self.output_dir.mkdir(exist_ok=True)
        (self.output_dir / "logs").mkdir(exist_ok=True)
        
        self.logger.info(f"í–¥ìƒëœ ë°°ì¹˜ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info(f"ì…ë ¥ ë””ë ‰í† ë¦¬: {self.input_dir}")
        self.logger.info(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {self.output_dir}")
        self.logger.info(f"ìµœëŒ€ ì›Œì»¤ ìˆ˜: {self.max_workers}")
        self.logger.info(f"GPU ì‚¬ìš©: {use_gpu}")
    
    def get_pdf_files(self) -> List[Path]:
        """PDF íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        pdf_files = list(self.input_dir.glob("*.pdf"))
        self.logger.info(f"ë°œê²¬ëœ PDF íŒŒì¼: {len(pdf_files)}ê°œ")
        return pdf_files
    
    def process_single_pdf(self, pdf_path: Path) -> Dict[str, Any]:
        """ë‹¨ì¼ PDF ì²˜ë¦¬"""
        result = {
            "filename": pdf_path.name,
            "filepath": str(pdf_path),
            "status": "success",
            "error": None,
            "summary": {},
            "processing_time": 0,
            "timestamp": time.time()
        }
        
        start_time = time.time()
        
        try:
            file_logger = self.logger
            output_subdir = self.output_dir / pdf_path.stem
            
            # í–¥ìƒëœ PDF ì²˜ë¦¬ê¸° ìƒì„±
            processor = EnhancedPDFProcessor(use_gpu=self.use_gpu)
            
            summary = processor.process_pdf_enhanced(
                pdf_path=str(pdf_path),
                output_dir=str(output_subdir)
            )
            
            result["summary"] = summary
            result["processing_time"] = time.time() - start_time
            
            log_file_processing(
                file_logger, 
                pdf_path.name, 
                "success", 
                f"ì²˜ë¦¬ ì‹œê°„: {result['processing_time']:.2f}ì´ˆ, "
                f"í…ìŠ¤íŠ¸ ë¸”ë¡: {summary.get('text_blocks_count', 0)}, "
                f"í…Œì´ë¸”: {summary.get('table_statistics', {}).get('total_tables', 0)}, "
                f"ì´ë¯¸ì§€: {summary.get('image_statistics', {}).get('total_images', 0)}",
                None
            )
            
            # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
            processor.cleanup()
            
        except Exception as e:
            result["status"] = "error"
            result["error"] = str(e)
            result["processing_time"] = time.time() - start_time
            
            error_log_path = self.output_dir / "logs" / f"error_{pdf_path.stem}.txt"
            with open(error_log_path, 'w', encoding='utf-8') as f:
                f.write(f"íŒŒì¼: {pdf_path.name}\n")
                f.write(f"ê²½ë¡œ: {pdf_path}\n")
                f.write(f"ì˜¤ë¥˜: {str(e)}\n")
                f.write(f"ì²˜ë¦¬ ì‹œê°„: {result['processing_time']:.2f}ì´ˆ\n")
                f.write(f"ìƒì„¸ ì •ë³´:\n{traceback.format_exc()}\n")
            
            log_file_processing(
                self.logger, 
                pdf_path.name, 
                "error", 
                f"ì²˜ë¦¬ ì‹œê°„: {result['processing_time']:.2f}ì´ˆ", 
                str(e)
            )
        
        return result
    
    def process_all_pdfs(self) -> Dict[str, Any]:
        """ëª¨ë“  PDF ì²˜ë¦¬"""
        pdf_files = self.get_pdf_files()
        
        if not pdf_files:
            self.logger.warning("ì²˜ë¦¬í•  PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return {"total_files": 0, "successful_files": 0, "failed_files": 0}
        
        self.logger.info(f"ì´ {len(pdf_files)}ê°œ PDF íŒŒì¼ ì²˜ë¦¬ ì‹œì‘")
        
        results = []
        
        # ë³‘ë ¬ ì²˜ë¦¬
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_file = {executor.submit(self.process_single_pdf, pdf_file): pdf_file 
                            for pdf_file in pdf_files}
            
            with tqdm(total=len(pdf_files), desc="PDF ì²˜ë¦¬ ì§„í–‰ë¥ ") as pbar:
                for future in concurrent.futures.as_completed(future_to_file):
                    result = future.result()
                    results.append(result)
                    pbar.update(1)
                    
                    if result["status"] == "success":
                        pbar.set_postfix({"ì„±ê³µ": len([r for r in results if r["status"] == "success"])})
                    else:
                        pbar.set_postfix({"ì‹¤íŒ¨": len([r for r in results if r["status"] == "error"])})
        
        # ê²°ê³¼ ìš”ì•½
        successful_files = [r for r in results if r["status"] == "success"]
        failed_files = [r for r in results if r["status"] == "error"]
        
        summary = {
            "total_files": len(pdf_files),
            "successful_files": len(successful_files),
            "failed_files": len(failed_files),
            "success_rate": len(successful_files) / len(pdf_files),
            "total_processing_time": sum(r["processing_time"] for r in results),
            "average_processing_time": sum(r["processing_time"] for r in results) / len(results),
            "results": results
        }
        
        # ë°°ì¹˜ ìš”ì•½ ì €ì¥
        batch_summary_path = self.output_dir / "enhanced_batch_summary.json"
        with open(batch_summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2, default=str)
        
        self._print_detailed_statistics(summary)
        return summary
    
    def _print_detailed_statistics(self, summary: Dict[str, Any]):
        """ìƒì„¸ í†µê³„ ì¶œë ¥"""
        self.logger.info("=" * 80)
        self.logger.info("í–¥ìƒëœ PDF íŒŒì‹± ê²°ê³¼ ìš”ì•½")
        self.logger.info("=" * 80)
        self.logger.info(f"ì´ íŒŒì¼ ìˆ˜: {summary['total_files']}")
        self.logger.info(f"ì„±ê³µí•œ íŒŒì¼ ìˆ˜: {summary['successful_files']}")
        self.logger.info(f"ì‹¤íŒ¨í•œ íŒŒì¼ ìˆ˜: {summary['failed_files']}")
        self.logger.info(f"ì„±ê³µë¥ : {summary['success_rate']:.2%}")
        self.logger.info(f"ì´ ì²˜ë¦¬ ì‹œê°„: {summary['total_processing_time']:.2f}ì´ˆ")
        self.logger.info(f"í‰ê·  ì²˜ë¦¬ ì‹œê°„: {summary['average_processing_time']:.2f}ì´ˆ")
        
        # ì„±ê³µí•œ íŒŒì¼ë“¤ì˜ ìƒì„¸ í†µê³„
        if summary['successful_files'] > 0:
            successful_results = [r for r in summary['results'] if r['status'] == 'success']
            
            total_text_blocks = sum(r['summary'].get('text_blocks_count', 0) for r in successful_results)
            total_tables = sum(r['summary'].get('table_statistics', {}).get('total_tables', 0) for r in successful_results)
            total_images = sum(r['summary'].get('image_statistics', {}).get('total_images', 0) for r in successful_results)
            
            self.logger.info(f"ì´ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ë¸”ë¡: {total_text_blocks}")
            self.logger.info(f"ì´ ì¶”ì¶œëœ í…Œì´ë¸”: {total_tables}")
            self.logger.info(f"ì´ ì¶”ì¶œëœ ì´ë¯¸ì§€: {total_images}")
        
        self.logger.info("=" * 80)

if __name__ == "__main__":
    # ì‚¬ìš© ì˜ˆì‹œ
    processor = EnhancedBatchProcessor(use_gpu=False)
    summary = processor.process_all_pdfs()
    print("ì²˜ë¦¬ ì™„ë£Œ!") 